_target_: plaid.vqvae.TransformerVQVAE

in_dim: 1024

vqvae_h_dim: 512
vqvae_res_h_dim: 512
vqvae_n_res_layers: 3
vqvae_n_embeddings: 1024
vqvae_kernel: 4
vqvae_stride: 2
vqvae_embedding_dim: 64
vqvae_beta: 0.25

patch_len: 16

transformer_hidden_act: "gelu"
transformer_intermediate_size: 3072
transformer_num_attention_heads: 16
transformer_num_hidden_layers: 12
transformer_position_embedding_type: "absolute"

lr: 1e-4
lr_beta1: 0.9
lr_beta2: 0.999
lr_num_warmup_steps: 0
lr_num_training_steps: 10_000_000