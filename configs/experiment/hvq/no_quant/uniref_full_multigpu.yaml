# @package _global_

defaults:
  - override /datamodule: uniref_fasta 

datamodule:
  batch_size: 64
  seq_len: 512 
  num_workers: 32

logger:
  name: no-quant

hourglass:
  seq_loss_weight: 0.0
  struct_loss_weight: 0.0
  log_sequence_loss: True 
  log_structure_loss: False
  downproj_factor: 128 
  shorten_factor: 1
  use_quantizer: False
  depth: 4 
  n_e: 512
  e_dim: 64
  updown_sample_type: "linear"

  lr: 3e-5 
  lr_sched_type: "cosine_with_restarts"
  lr_num_warmup_steps: 5000
  lr_num_training_steps: 1_000_000
  lr_num_cycles: 2

# 100000000/128 = 781250

trainer:
  precision: "bf16-mixed"
  log_every_n_steps: 50
  gradient_clip_val: 0.5

callbacks/compression:
  run_every_n_steps: 10000
