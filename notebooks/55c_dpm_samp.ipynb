{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be77fb2-2be4-4ef5-bcff-c337ac2e0f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5d6a4-43cc-43cd-b2cd-32d85ba411eb",
   "metadata": {},
   "source": [
    "# Load EMA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40340dd1-691e-46a9-a7a4-89764ae70232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homefs/home/lux70/micromamba/envs/plaid-train/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/homefs/home/lux70/micromamba/envs/plaid-train/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "from plaid.diffusion import FunctionOrganismDiffusion\n",
    "from plaid.denoisers import FunctionOrganismUDiT, DenoiserKwargs\n",
    "from plaid.constants import COMPRESSION_INPUT_DIMENSIONS, COMPRESSION_SHORTEN_FACTORS\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41fe821-9853-4dac-8770-7ad43913f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"5j007z42\"\n",
    "\n",
    "ckpt_dir = Path(\"/data/lux70/plaid/checkpoints/plaid-compositional\") \n",
    "model_path = ckpt_dir / model_id / \"last.ckpt\"\n",
    "config_path = ckpt_dir / model_id / \"config.yaml\"\n",
    "\n",
    "cfg = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0ee233-db93-413c-9ad8-3d64002782ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_model_id = cfg['compression_model_id']\n",
    "shorten_factor = COMPRESSION_SHORTEN_FACTORS[compression_model_id]\n",
    "input_dim = COMPRESSION_INPUT_DIMENSIONS[compression_model_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ccb6306-1595-415f-972a-f2e26c8f0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser_kwargs = cfg.denoiser\n",
    "denoiser_kwargs.pop(\"_target_\")\n",
    "denoiser = FunctionOrganismUDiT(**denoiser_kwargs, input_dim=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893d53c5-fb33-4ba3-abf0-f5fcef079c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181578/2446128762.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lask.ckpt automatically links to the EMA\n",
    "\n",
    "ckpt = torch.load(model_path)\n",
    "ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339644e5-2ad5-4c13-a861-b0451aa896dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_state_dict = {}\n",
    "for k, v in ckpt['state_dict'].items():\n",
    "    if k[:16] == \"model._orig_mod.\":\n",
    "        mod_state_dict[k[16:]] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db302af-0823-4253-87bd-2759f57bba5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoiser.load_state_dict(mod_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de95b1ca-a6a7-4577-a627-3a4e440d28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_kwargs = cfg.diffusion\n",
    "diffusion_kwargs.pop(\"_target_\")\n",
    "\n",
    "# diffusion_kwargs['beta_scheduler_name'] = \"sigmoid\"\n",
    "# diffusion_kwargs['sampling_timesteps'] = 500\n",
    "\n",
    "diffusion = FunctionOrganismDiffusion(model=denoiser,**diffusion_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11c8b285-5450-4c5b-a524-5bd16104f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homefs/home/lux70/code/cheap-proteins/src/cheap/pretrained.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_fpath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tanh layer at bottleneck...\n",
      "Finished loading HPCT model with shorten factor 2 and 32 channel dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homefs/home/lux70/code/cheap-proteins/src/cheap/decoder.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path)\n",
      "Creating ESMFold...\n",
      "ESMFold model loaded in 33.80 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cheap.proteins.LatentToStructure at 0x7fa07c1fbc70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cheap.pretrained import load_model_from_id\n",
    "cheap_model = load_model_from_id(compression_model_id)\n",
    "_ = cheap_model.to(device)\n",
    "\n",
    "from cheap.proteins import LatentToSequence,LatentToStructure\n",
    "latent_to_sequence = LatentToSequence()\n",
    "latent_to_sequence.to(device)\n",
    "\n",
    "latent_to_structure = LatentToStructure()\n",
    "latent_to_structure.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da3b78-1ae3-43d5-bcfb-e1debeeb2b88",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dbb148c-1816-460f-a511-050f4b9ac60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organism_idx = org_df[org_df.organism_id == \"HUMAN\"].organism_index.iloc[0]\n",
    "# function_idx = go_df[go_df.GO_term == \"carbohydrate metabolic process\"].GO_idx.iloc[0]\n",
    "# print(organism_idx, function_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef5574ef-a2a4-4ed4-a8c0-ef6046ded4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "diffusion = diffusion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbfd8cdc-0b38-4e42-9896-9bceae605613",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 78.30 GiB is allocated by PyTorch, and 320.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction_idx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m function_idx\n\u001b[1;32m     19\u001b[0m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcond_scale\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cond_scale\n\u001b[0;32m---> 21\u001b[0m solver \u001b[38;5;241m=\u001b[39m \u001b[43mSampleLatent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/code/plaid/src/plaid/pipeline/_sample.py:127\u001b[0m, in \u001b[0;36mSampleLatent.__init__\u001b[0;34m(self, model_id, model_ckpt_dir, organism_idx, function_idx, cond_scale, num_samples, beta_scheduler_name, beta_scheduler_start, beta_scheduler_end, beta_scheduler_tau, sampling_timesteps, batch_size, length, return_all_timesteps, output_root_dir, sample_scheduler, sigma_min, sigma_max)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_scheduler_tau \u001b[38;5;241m=\u001b[39m default(\n\u001b[1;32m    123\u001b[0m     beta_scheduler_tau, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39mbeta_scheduler_tau\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# create the denoiser and solvers\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenoiser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_denoiser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_diffusion(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenoiser)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenoiser\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/code/plaid/src/plaid/pipeline/_sample.py:155\u001b[0m, in \u001b[0;36mSampleLatent.create_denoiser\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown denoiser class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdenoiser_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# lask.ckpt automatically links to the EMA\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# remove the prefix from the state dict if torch.compile was used during training\u001b[39;00m\n\u001b[1;32m    158\u001b[0m mod_state_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/micromamba/envs/plaid-train/lib/python3.10/site-packages/torch/serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/plaid-train/lib/python3.10/site-packages/torch/serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[1;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/micromamba/envs/plaid-train/lib/python3.10/site-packages/torch/serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/micromamba/envs/plaid-train/lib/python3.10/site-packages/torch/serialization.py:1466\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1461\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1466\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1467\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1468\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1471\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/micromamba/envs/plaid-train/lib/python3.10/site-packages/torch/serialization.py:414\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/micromamba/envs/plaid-train/lib/python3.10/site-packages/torch/serialization.py:392\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m    391\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/plaid-train/lib/python3.10/site-packages/torch/storage.py:187\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice, non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:  \u001b[38;5;66;03m# type: ignore[type-var, misc] # noqa: E704\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/plaid-train/lib/python3.10/site-packages/torch/_utils.py:89\u001b[0m, in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sparse\n\u001b[1;32m     88\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 89\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 45.62 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 78.30 GiB is allocated by PyTorch, and 320.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from plaid.datasets import NUM_ORGANISM_CLASSES, NUM_FUNCTION_CLASSES\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from plaid.pipeline import SampleLatent\n",
    "\n",
    "cfg = OmegaConf.load(\"/homefs/home/lux70/code/plaid/configs/pipeline/sample_latent.yaml\")\n",
    "\n",
    "N, L = 64, 88\n",
    "assert L % 4 == 0\n",
    "shape = (N, L, input_dim)\n",
    "\n",
    "organism_idx = NUM_ORGANISM_CLASSES\n",
    "function_idx = 28  # protein kinase activity\n",
    "cond_scale = 8.\n",
    "\n",
    "cfg['sample_scheduler'] = \"dpmpp_3m_sde\"\n",
    "cfg['sampling_timesteps'] = 30\n",
    "cfg['function_idx'] = function_idx\n",
    "cfg['cond_scale'] = cond_scale\n",
    "\n",
    "solver = SampleLatent(**cfg)\n",
    "x = solver.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc49c1b-4349-43c0-bbc2-86bc37c22029",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampled_latent.shape)\n",
    "final_sample = sampled_latent[:, -1, :, :]\n",
    "print(final_sample.shape)\n",
    "\n",
    "print(final_sample.max(), final_sample.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3f51f-c486-4b19-b8e9-449c810ddad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_uncompressed = cheap_model.decode(final_sample, downsampled_mask=None)\n",
    "print(sampled_uncompressed.min(), sampled_uncompressed.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf8ec3-7afc-48ae-b0d5-d19ded99b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "# with open(\"test_sample.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(sampled_uncompressed,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e77d77c-ee26-41ee-a29b-67b3c9777136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cheap.utils import LatentScaler\n",
    "latent_scaler = LatentScaler()\n",
    "sampled_unscaled = latent_scaler.unscale(sampled_uncompressed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36492297-c8cc-44f2-8198-88b8e1223b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampled_unscaled.shape)\n",
    "print(sampled_unscaled.max(), sampled_unscaled.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf6908-d86b-4266-a9a9-65c020f2c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = latent_to_sequence.to_sequence(sampled_unscaled)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e3c4f-692f-435d-b495-d842e2653577",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c6666-1bee-41b8-a7be-5b408aa3aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_strs, raw_outputs = latent_to_structure.to_structure(sampled_unscaled, return_raw_outputs=True, sequences=sequences, batch_size=32, num_recycles=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad56f46-1950-495f-9884-6575c0e9a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "_ = sns.distplot(raw_outputs['plddt'].mean(dim=-1).mean(dim=-1).cpu().numpy(), bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87165734-415d-47f3-ad0f-9edbb69a8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "\n",
    "# for i in range(len(pdb_strs)):\n",
    "# for i in range(10,20): \n",
    "# for i in range(0, 10): \n",
    "for i in range(20,30): \n",
    "    view = py3Dmol.view(width=600, height=600)\n",
    "    view.addModelsAsFrames(pdb_strs[i])\n",
    "    \n",
    "    # Apply the plDDT color scheme\n",
    "    # view.setStyle({'cartoon': {'color': {'prop': 'b', 'gradient': 'roygb', 'min': 0, 'max': 100}}})\n",
    "    view.setStyle({'cartoon': {'color': {'prop': 'b', 'gradient': 'roygb', 'min': 50, 'max': 90}}})\n",
    "    \n",
    "    # # Add surface representation with plDDT-based color\n",
    "    view.addSurface(py3Dmol.VDW, {'opacity': 0.7, 'colorscheme': {'prop': 'b', 'gradient': 'roygb', 'min': 50, 'max': 90}})\n",
    "    # view.addSurface(py3Dmol.VDW, {'opacity': 0.7, 'colorscheme': {'prop': 'b', 'gradient': 'roygb', 'min': 0, 'max': 100}})\n",
    "\n",
    "    view.zoomTo()\n",
    "    view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f4adf-d61c-47e7-afa6-08619861e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plaid.evaluation import RITAPerplexity\n",
    "\n",
    "perplexity_calc = RITAPerplexity(device=device)\n",
    "perplexities = perplexity_calc.batch_eval(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3cd6d-9fbb-48b8-bb2e-e824f985020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = [perplexity_calc.calc_perplexity(s) for s in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db437c2-58e1-4121-9bdc-7d57f6353864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "_ = sns.distplot(perplexities, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6a69e-c3c4-4bca-b5c5-59f367b7abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89dcc9-dc2f-4cf5-82af-d3a1b6e61a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plaid.utils import write_to_fasta\n",
    "\n",
    "\n",
    "write_to_fasta(\n",
    "    sequences,\n",
    "    \"/homefs/home/lux70/generated.fasta\",\n",
    "    [f\"sample{i}\" for i in range(len(sequences))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba241a5-a334-4034-aabd-186e81be9028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020b50b-00e5-4e6e-8cc4-71c5db80b9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9d17c-ab42-444e-a5a3-61a868e2a0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373a1b7-5ca1-481b-9b17-bc3c54fc7c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbddd6f-45e6-4b9f-8445-f5c030e3e829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34723cc-ab4a-4a27-8031-55871fd4ca76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422551f4-18cd-4c94-82e2-ab314664b5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17409f0f-368a-4514-8e5d-5b930dbda11d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plaid-train)",
   "language": "python",
   "name": "plaid-train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
