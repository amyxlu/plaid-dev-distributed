{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbac56de-4e4f-4065-9057-dca074a1f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e53f155-d9d7-42f9-9b44-fb00296a2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from omegaconf import ListConfig\n",
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866897ec-6b94-4b3f-bada-aaad9c83b0ec",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765904d5-50d6-40b5-8de3-736bad9cd515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as T\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, reduce, repeat\n",
    "from torch.optim import AdamW\n",
    "import lightning as L\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from omegaconf import ListConfig\n",
    "\n",
    "from plaid.datasets import CATHShardedDataModule\n",
    "from plaid.transforms import trim_or_pad_batch_first\n",
    "from plaid.esmfold.misc import batch_encode_sequences\n",
    "from plaid.utils import LatentScaler, get_lr_scheduler\n",
    "from plaid.proteins import LatentToSequence, LatentToStructure\n",
    "from plaid.losses.modules import SequenceAuxiliaryLoss, BackboneAuxiliaryLoss\n",
    "from plaid.losses.functions import masked_token_accuracy, masked_token_cross_entropy_loss\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def pad_to_multiple(tensor, multiple, dim = -1, value = 0):\n",
    "    seq_len = tensor.shape[dim]\n",
    "    m = seq_len / multiple\n",
    "    if m.is_integer():\n",
    "        return tensor\n",
    "    remainder = math.ceil(m) * multiple - seq_len\n",
    "    pad_offset = (0,) * (-1 - dim) * 2\n",
    "    return F.pad(tensor, (*pad_offset, 0, remainder), value = value)\n",
    "\n",
    "def cast_tuple(val, depth = 1):\n",
    "    return val if isinstance(val, tuple) else ((val,) * depth)\n",
    "\n",
    "# factory\n",
    "def _valid_depth_dtype(depth):\n",
    "    if isinstance(depth, int):\n",
    "        return True\n",
    "    if isinstance(depth, tuple) or isinstance(depth, list) or isinstance(depth, ListConfig):\n",
    "        if len(depth) == 3:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _check_if_nest(var):\n",
    "    return isinstance(var, (tuple, list, ListConfig)) and len(var) > 0\n",
    "\n",
    "# up and down sample classes\n",
    "\n",
    "class NaiveDownsample(nn.Module):\n",
    "    def __init__(self, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return reduce(x, 'b (n s) d -> b n d', 'mean', s = self.shorten_factor)\n",
    "\n",
    "class NaiveUpsample(nn.Module):\n",
    "    def __init__(self, elongate_factor):\n",
    "        super().__init__()\n",
    "        self.elongate_factor = elongate_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return repeat(x, 'b n d -> b (n s) d', s = self.elongate_factor)\n",
    "\n",
    "class LinearDownsample(nn.Module):\n",
    "    def __init__(self, dim, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim * shorten_factor, dim)\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b (n s) d -> b n (s d)', s = self.shorten_factor)\n",
    "        return self.proj(x)\n",
    "\n",
    "class LinearUpsample(nn.Module):\n",
    "    def __init__(self, dim, elongate_factor):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, dim * elongate_factor)\n",
    "        self.elongate_factor = elongate_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return rearrange(x, 'b n (s d) -> b (n s) d', s = self.elongate_factor)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNormLinearDownProjection(nn.Module):\n",
    "    def __init__(self, dim, downproj_factor):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.proj = nn.Linear(dim, dim // downproj_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(self.norm(x))\n",
    "\n",
    "class PreNormLinearUpProjection(nn.Module):\n",
    "    def __init__(self, dim, upproj_factor):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.proj = nn.Linear(dim, dim * upproj_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(self.norm(x))\n",
    "\n",
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs) + x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        causal = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = heads * dim_head\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h, device = self.heads, x.device\n",
    "        kv_input = default(context, x)\n",
    "\n",
    "        q, k, v = self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "        mask_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b j -> b () () j')\n",
    "            sim = sim.masked_fill(~mask, mask_value)\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = sim.shape[-2:]\n",
    "            mask = torch.ones(i, j, device = device, dtype = torch.bool).triu_(j - i + 1)\n",
    "            mask = rearrange(mask, 'i j -> () () i j')\n",
    "            sim = sim.masked_fill(mask, mask_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "def FeedForward(dim, mult = 4, dropout = 0.):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, dim * mult),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(dim * mult, dim)\n",
    "    )\n",
    "\n",
    "# transformer classes\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth,\n",
    "        causal = False,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_mult = 4,\n",
    "        ff_dropout = 0.,\n",
    "        norm_out = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNormResidual(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout, causal = causal)),\n",
    "                PreNormResidual(dim, FeedForward(dim, mult = ff_mult, dropout = ff_dropout))\n",
    "            ]))\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, compressed = None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, context = context, mask = mask)\n",
    "            x = ff(x)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80330cab-90a8-46f2-932f-4e3b8ba3f46c",
   "metadata": {},
   "source": [
    "# Hourglass Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff35672-8543-477d-a7dc-665541df2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourglassEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth=(4, 4),\n",
    "        shorten_factor = (2, 2),\n",
    "        downproj_factor = (2, 2),\n",
    "        attn_resampling = True,\n",
    "        updown_sample_type = 'naive',\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        causal = False,\n",
    "        norm_out = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # shorten factor\n",
    "        if isinstance(shorten_factor, (tuple, list, ListConfig)):\n",
    "            assert len(depth) == len(shorten_factor) == len(downproj_factor)\n",
    "            shorten_factor, *rest_shorten_factor = shorten_factor\n",
    "        elif isinstance(shorten_factor, int):\n",
    "            shorten_factor, rest_shorten_factor = shorten_factor, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        # downproj factor\n",
    "        if isinstance(downproj_factor, (tuple, list, ListConfig)):\n",
    "            downproj_factor, *rest_downproj_factor = downproj_factor\n",
    "        elif isinstance(downproj_factor, int):\n",
    "            downproj_factor, rest_downproj_factor = downproj_factor, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        # depth \n",
    "        if isinstance(depth, (tuple, list, ListConfig)):\n",
    "            depth, *rest_depth = depth\n",
    "        elif isinstance(depth, int):\n",
    "            depth, rest_depth = depth, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        # shared transformer kwargs\n",
    "        transformer_kwargs = dict(\n",
    "            heads = heads,\n",
    "            dim_head = dim_head\n",
    "        )\n",
    "\n",
    "        self.causal = causal\n",
    "        self.shorten_factor = shorten_factor\n",
    "        self.downproj_factor = downproj_factor\n",
    "\n",
    "        if updown_sample_type == 'naive':\n",
    "            self.downsample = NaiveDownsample(shorten_factor)\n",
    "        elif updown_sample_type == 'linear':\n",
    "            self.downsample = LinearDownsample(dim, shorten_factor)\n",
    "        else:\n",
    "            raise ValueError(f'unknown updown_sample_type keyword value - must be either naive or linear for now')\n",
    "\n",
    "        self.down_projection = PreNormLinearDownProjection(dim, downproj_factor)\n",
    "        if _check_if_nest(rest_depth):\n",
    "            assert _check_if_nest(rest_shorten_factor)\n",
    "            assert _check_if_nest(rest_downproj_factor)\n",
    "            self.nested_encoder = HourglassEncoder(\n",
    "                dim = dim // downproj_factor,\n",
    "                shorten_factor = rest_shorten_factor,\n",
    "                downproj_factor = rest_downproj_factor,\n",
    "                depth = rest_depth,\n",
    "                attn_resampling = attn_resampling,\n",
    "                updown_sample_type = updown_sample_type,\n",
    "                causal = causal,\n",
    "                **transformer_kwargs\n",
    "            )\n",
    "            self.has_nest = True \n",
    "        else:\n",
    "            self.has_nest = False\n",
    "\n",
    "        self.pre_transformer = Transformer(dim = dim, depth = depth, causal = causal, **transformer_kwargs)\n",
    "        self.attn_resampling_pre_valley = Transformer(dim = dim, depth = 1, **transformer_kwargs) if attn_resampling else None\n",
    "        self.norm_out = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        \"\"\"\n",
    "        x: input\n",
    "        mask: indicates if input has a padding (True if we should keep it, False if it's padding & we should discard it)\n",
    "        compressed: at the start, should be None; if it's already populated, ignore further actions at the valley step\n",
    "        \"\"\"\n",
    "        if x.shape[1] % self.shorten_factor != 0:\n",
    "            raise ValueError(\"Length of input `x` must be a multiple of `self.shorten_factor` for attention resampling\")\n",
    "\n",
    "        # b : batch, n : sequence length, d : feature dimension, s : shortening factor\n",
    "        s, b, n = self.shorten_factor, *x.shape[:2]\n",
    "\n",
    "        # top half of hourglass, pre-transformer layers\n",
    "        x = self.pre_transformer(x, mask = mask)\n",
    "\n",
    "        # pad to multiple of shortening factor, in preparation for pooling\n",
    "        x = pad_to_multiple(x, s, dim = -2)\n",
    "\n",
    "        # print(mask)\n",
    "        if exists(mask):\n",
    "            padded_mask = pad_to_multiple(mask, s, dim = -1, value = False)\n",
    "        # print(padded_mask)\n",
    "            \n",
    "\n",
    "        # save the residual, and for \"attention resampling\" at downsample and upsample\n",
    "        x_residual = x.clone()\n",
    "\n",
    "        # if autoregressive, do the shift by shortening factor minus one\n",
    "        if self.causal:\n",
    "            shift = s - 1\n",
    "            x = F.pad(x, (0, 0, shift, -shift), value = 0.)\n",
    "\n",
    "            if exists(mask):\n",
    "                padded_mask = F.pad(padded_mask, (shift, -shift), value = False)\n",
    "\n",
    "        # naive average pool along length dimension\n",
    "        downsampled = self.downsample(x)\n",
    "        if exists(mask):\n",
    "            downsampled_mask = reduce(padded_mask, 'b (n s) -> b n', 'sum', s = s) > 0\n",
    "        else:\n",
    "            downsampled_mask = None\n",
    "        # print('down sampled mask', downsampled_mask)\n",
    "\n",
    "        # pre-valley \"attention resampling\" - they have the pooled token in each bucket attend to the tokens pre-pooled\n",
    "        if exists(self.attn_resampling_pre_valley):\n",
    "            if exists(mask):\n",
    "                attn_resampling_mask = rearrange(padded_mask, 'b (n s) -> (b n) s', s = s)\n",
    "            else:\n",
    "                attn_resampling_mask = None\n",
    "            downsampled = self.attn_resampling_pre_valley(\n",
    "                rearrange(downsampled, 'b n d -> (b n) () d'),\n",
    "                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n",
    "                mask = attn_resampling_mask\n",
    "            )\n",
    "\n",
    "            downsampled = rearrange(downsampled, '(b n) () d -> b n d', b = b)\n",
    "            \n",
    "        # also possibly reduce along dim=-1\n",
    "        out = self.down_projection(downsampled)\n",
    "\n",
    "        # the \"valley\" - either a regular transformer or another hourglass\n",
    "        if self.has_nest: \n",
    "            out, downsampled_mask = self.nested_encoder(out, mask = downsampled_mask)\n",
    "        \n",
    "        return self.norm_out(out), downsampled_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f17d8d-5684-40f2-882e-dec7799339c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortenf = 2\n",
    "downprojf = 2\n",
    "enc = HourglassEncoder(\n",
    "    dim=1024,\n",
    "    depth=4,\n",
    "    shorten_factor=shortenf,\n",
    "    downproj_factor=downprojf,\n",
    "    attn_resampling=True,\n",
    "    updown_sample_type=\"linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1fb2e80-e3a7-4d71-9683-984b7ceb48a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "enc.to(device)\n",
    "emb = torch.randn((4,8,1024)).to(device)\n",
    "mask = einops.repeat(torch.tensor([1,1,1,1,1,0,0,0]), \"l -> n l\", n=4).bool().to(device)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3e6c4e-96cb-488a-91aa-3acebe828a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ze, downsampled_mask = enc(emb,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21431a4-288a-4bb4-ac1e-f6e0cd4b4617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 512])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(ze.shape)\n",
    "print(downsampled_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b0000c-14b3-4d22-a964-7c69fcf136bd",
   "metadata": {},
   "source": [
    "# Quantizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54063a2-f80e-4239-aa92-9ac84038e39e",
   "metadata": {},
   "source": [
    "## Vector-Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f8f0c1d-cbbe-4fbd-a552-a27af4c6b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def expand_to_shape(x, target_shape):\n",
    "    # keep adding dimensions to the end until we match target dimensions\n",
    "    while len(x.shape) < len(target_shape):\n",
    "        x = x[..., None]\n",
    "    return x.expand(target_shape)\n",
    "\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Discretization bottleneck part of the VQ-VAE.\n",
    "\n",
    "    Inputs:\n",
    "    - n_e : number of embeddings\n",
    "    - e_dim : dimension of embedding\n",
    "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, e_dim, beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.n_e = n_e\n",
    "        self.e_dim = e_dim\n",
    "        self.beta = beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Inputs the output of the encoder network z and maps it to a discrete \n",
    "        one-hot vector that is the index of the closest embedding vector e_j\n",
    "\n",
    "        z (continuous) -> z_q (discrete)\n",
    "\n",
    "        z.shape = (batch, length, channel)\n",
    "\n",
    "        quantization pipeline:\n",
    "\n",
    "            1. get encoder input (B,L,C)\n",
    "            2. flatten input to (B*L,C)\n",
    "\n",
    "        \"\"\"\n",
    "        # reshape z -> (batch, height, channel) and flatten\n",
    "        z_flattened = einops.rearrange(z, \"b l c -> (b l) c\").view(-1, self.e_dim)\n",
    "        device = z.device\n",
    "\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
    "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
    "            torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "\n",
    "        # find closest encodings\n",
    "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
    "        min_encodings = torch.zeros(\n",
    "            min_encoding_indices.shape[0], self.n_e).to(device)\n",
    "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
    "\n",
    "        # compute loss for embedding\n",
    "        # embedding_loss = masked_mse_loss(z_q.detach(), z, mask)\n",
    "        # commitment_loss = masked_mse_loss(z_q, z.detach(), mask)\n",
    "        # loss = embedding_loss + self.beta * commitment_loss\n",
    "        loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
    "            torch.mean((z_q - z.detach()) ** 2)\n",
    "\n",
    "        # preserve gradients\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # perplexity\n",
    "        e_mean = torch.mean(min_encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"z_q\": z_q,\n",
    "            \"perplexity\": perplexity,\n",
    "            \"min_encodings\": min_encodings,\n",
    "            \"min_encoding_indices\": min_encoding_indices\n",
    "        }\n",
    "\n",
    "    def get_codebook_entry(self, indices, shape=None):\n",
    "        # shape specifying (batch, length, channel)\n",
    "        # TODO: check for more easy handling with nn.Embedding\n",
    "        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n",
    "        min_encodings.scatter_(1, indices[:,None], 1)\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n",
    "\n",
    "        if shape is not None:\n",
    "            z_q = z_q.view(shape)\n",
    "        return z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29713cb9-5aeb-426e-a1db-6e7f060049b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_e = 16\n",
    "e_dim = 64\n",
    "quantizer = VectorQuantizer(n_e, e_dim, 0.25)\n",
    "quantizer = quantizer.to(device)\n",
    "quant_outputs = quantizer(ze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d1fea3f-4bc7-4914-b11b-949b94107509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss torch.Size([])\n",
      "z_q torch.Size([4, 4, 512])\n",
      "perplexity torch.Size([])\n",
      "min_encodings torch.Size([128, 16])\n",
      "min_encoding_indices torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "for k, v in quant_outputs.items():\n",
    "    print(k, v.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97e7ac93-700a-4fc8-955a-b7d474db1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_q = quant_outputs['z_q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b151d52b-e5f9-471d-bcda-97e9f273375e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbiklEQVR4nO3dfWxV9f3A8U+hcmGkVGHjoZNKZ4xOYKhDjGI2jGSEMDazqHNBbDDRmVURuzCoGzrmQ8Vt/jqVgJpsukR8+EOY00zDGMrMRB46nGYTISJ2GmBm2kqJHWnP74/FZhVE0XO/l5bXKzl/3HNP7/dz7HZ559x7e8uyLMsCACCRfqUeAAA4uogPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBIqrzUA3xYV1dXvPXWW1FRURFlZWWlHgcA+ASyLIv33nsvqqqqol+/Q1/bOOLi46233orRo0eXegwA4FNoaWmJ448//pDHHHHxUVFRERH/HX7IkCElngYA+CTa2tpi9OjR3f+OH8oRFx8fvNQyZMgQ8QEAvcwnecuEN5wCAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AIKnDjo9169bFzJkzo6qqKsrKymLVqlUfeexVV10VZWVl0dTU9BlGBAD6ksOOj/b29pgwYUIsXbr0kMetXLky1q9fH1VVVZ96OACg7znsL5abPn16TJ8+/ZDHvPnmm3HNNdfE008/HTNmzPjUwwEAfU/u32rb1dUVs2fPjvnz58fYsWM/9viOjo7o6Ojovt3W1pb3SADAEST3+FiyZEmUl5fH3LlzP9HxjY2NsXjx4rzHIJExC59Mvubrt7maBtCb5fppl82bN8evfvWruP/++6OsrOwT/UxDQ0O0trZ2by0tLXmOBAAcYXKNjz//+c+xZ8+eqK6ujvLy8igvL4+dO3fGD3/4wxgzZsxBf6ZQKMSQIUN6bABA35Xryy6zZ8+OqVOn9tg3bdq0mD17dsyZMyfPpQCAXuqw42Pv3r2xffv27ts7duyILVu2xNChQ6O6ujqGDRvW4/hjjjkmRo4cGSeffPJnnxYA6PUOOz42bdoU5513Xvft+vr6iIiora2N+++/P7fBAIC+6bDjY8qUKZFl2Sc+/vXXXz/cJQCAPsx3uwAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBShx0f69ati5kzZ0ZVVVWUlZXFqlWruu/bv39/LFiwIMaPHx+DBw+OqqqquOyyy+Ktt97Kc2YAoBc77Phob2+PCRMmxNKlSw+4b9++fdHc3ByLFi2K5ubmeOyxx2Lr1q3xrW99K5dhAYDer/xwf2D69Okxffr0g95XWVkZq1ev7rHv7rvvjkmTJsUbb7wR1dXVn25KAKDPKPp7PlpbW6OsrCyOPfbYYi8FAPQCh33l43C8//77sWDBgvje974XQ4YMOegxHR0d0dHR0X27ra2tmCMBACVWtPjYv39/XHzxxZFlWSxbtuwjj2tsbIzFixcXawwAcjZm4ZNJ13v9thlJ16P4ivKyywfhsXPnzli9evVHXvWIiGhoaIjW1tburaWlpRgjAQBHiNyvfHwQHtu2bYu1a9fGsGHDDnl8oVCIQqGQ9xgAwBHqsONj7969sX379u7bO3bsiC1btsTQoUNj1KhRceGFF0Zzc3M88cQT0dnZGbt27YqIiKFDh8aAAQPymxwA6JUOOz42bdoU5513Xvft+vr6iIiora2Nn/70p/H4449HRMRpp53W4+fWrl0bU6ZM+fSTAgB9wmHHx5QpUyLLso+8/1D3AQD4bhcAICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEkddnysW7cuZs6cGVVVVVFWVharVq3qcX+WZXHDDTfEqFGjYtCgQTF16tTYtm1bXvMCAL3cYcdHe3t7TJgwIZYuXXrQ+2+//fa48847Y/ny5fHCCy/E4MGDY9q0afH+++9/5mEBgN6v/HB/YPr06TF9+vSD3pdlWTQ1NcVPfvKT+Pa3vx0REb/97W9jxIgRsWrVqrjkkks+27QAQK+X63s+duzYEbt27YqpU6d276usrIyzzjornn/++YP+TEdHR7S1tfXYAIC+67CvfBzKrl27IiJixIgRPfaPGDGi+74Pa2xsjMWLF+c5BnAEG7PwyVKPAJRYyT/t0tDQEK2trd1bS0tLqUcCAIoo1/gYOXJkRETs3r27x/7du3d33/dhhUIhhgwZ0mMDAPquXOOjpqYmRo4cGWvWrOne19bWFi+88EKcffbZeS4FAPRSh/2ej71798b27du7b+/YsSO2bNkSQ4cOjerq6pg3b17cfPPNcdJJJ0VNTU0sWrQoqqqq4oILLshzbgCglzrs+Ni0aVOcd9553bfr6+sjIqK2tjbuv//++NGPfhTt7e1x5ZVXxrvvvhvnnntuPPXUUzFw4MD8pgYAeq3Djo8pU6ZElmUfeX9ZWVn87Gc/i5/97GefaTAAoG8q+addAICji/gAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkiov9QB93ZiFTyZd7/XbZiRdD6DYjobn0aPhHP+XKx8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACCp3OOjs7MzFi1aFDU1NTFo0KA48cQT46abboosy/JeCgDohcrzfsAlS5bEsmXL4oEHHoixY8fGpk2bYs6cOVFZWRlz587NezkAoJfJPT7+8pe/xLe//e2YMWNGRESMGTMmHnroodiwYUPeSwEAvVDuL7ucc845sWbNmnj11VcjIuLFF1+M5557LqZPn37Q4zs6OqKtra3HBgD0Xblf+Vi4cGG0tbXFKaecEv3794/Ozs645ZZbYtasWQc9vrGxMRYvXpz3GADAESr3Kx+PPvpoPPjgg7FixYpobm6OBx54IH7xi1/EAw88cNDjGxoaorW1tXtraWnJeyQA4AiS+5WP+fPnx8KFC+OSSy6JiIjx48fHzp07o7GxMWpraw84vlAoRKFQyHsMAOAIlfuVj3379kW/fj0ftn///tHV1ZX3UgBAL5T7lY+ZM2fGLbfcEtXV1TF27Nj461//GnfccUdcfvnleS8FAPRCucfHXXfdFYsWLYof/OAHsWfPnqiqqorvf//7ccMNN+S9FADQC+UeHxUVFdHU1BRNTU15PzQA0Af4bhcAICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACRVXuoBgJ7GLHyy1CMAFJUrHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AIKmixMebb74Zl156aQwbNiwGDRoU48ePj02bNhVjKQCglynP+wHfeeedmDx5cpx33nnxhz/8Ib7whS/Etm3b4rjjjst7KQCgF8o9PpYsWRKjR4+O3/zmN937ampq8l4GAOilcn/Z5fHHH4+JEyfGRRddFMOHD4/TTz897rvvvo88vqOjI9ra2npsAEDflfuVj9deey2WLVsW9fX1cf3118fGjRtj7ty5MWDAgKitrT3g+MbGxli8eHHeYxy1xix8stQj9Dn+mwLkK/crH11dXXHGGWfErbfeGqeffnpceeWVccUVV8Ty5csPenxDQ0O0trZ2by0tLXmPBAAcQXKPj1GjRsWpp57aY9+Xv/zleOONNw56fKFQiCFDhvTYAIC+K/f4mDx5cmzdurXHvldffTVOOOGEvJcCAHqh3OPjuuuui/Xr18ett94a27dvjxUrVsS9994bdXV1eS8FAPRCucfHmWeeGStXroyHHnooxo0bFzfddFM0NTXFrFmz8l4KAOiFcv+0S0TEN7/5zfjmN79ZjIcGAHo53+0CACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACCposfHbbfdFmVlZTFv3rxiLwUA9AJFjY+NGzfGPffcE1/5yleKuQwA0IsULT727t0bs2bNivvuuy+OO+64Yi0DAPQyRYuPurq6mDFjRkydOvWQx3V0dERbW1uPDQDou8qL8aAPP/xwNDc3x8aNGz/22MbGxli8eHExxjioMQufTLYWAL2PfyeKL/crHy0tLXHttdfGgw8+GAMHDvzY4xsaGqK1tbV7a2lpyXskAOAIkvuVj82bN8eePXvijDPO6N7X2dkZ69ati7vvvjs6Ojqif//+3fcVCoUoFAp5jwEAHKFyj4/zzz8/XnrppR775syZE6ecckosWLCgR3gAAEef3OOjoqIixo0b12Pf4MGDY9iwYQfsBwCOPv7CKQCQVFE+7fJhzzzzTIplAIBewJUPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASCr3+GhsbIwzzzwzKioqYvjw4XHBBRfE1q1b814GAOilco+PZ599Nurq6mL9+vWxevXq2L9/f3zjG9+I9vb2vJcCAHqh8rwf8Kmnnupx+/7774/hw4fH5s2b42tf+1reywEAvUzu8fFhra2tERExdOjQg97f0dERHR0d3bfb2tqKPRIAUEJFfcNpV1dXzJs3LyZPnhzjxo076DGNjY1RWVnZvY0ePbqYIwEAJVbU+Kirq4uXX345Hn744Y88pqGhIVpbW7u3lpaWYo4EAJRY0V52ufrqq+OJJ56IdevWxfHHH/+RxxUKhSgUCsUaAwA4wuQeH1mWxTXXXBMrV66MZ555JmpqavJeAgDoxXKPj7q6ulixYkX87ne/i4qKiti1a1dERFRWVsagQYPyXg4A6GVyf8/HsmXLorW1NaZMmRKjRo3q3h555JG8lwIAeqGivOwCAPBRfLcLAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSKi/1AHC4xix8stQjAPAZuPIBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSKlp8LF26NMaMGRMDBw6Ms846KzZs2FCspQCAXqQo8fHII49EfX193HjjjdHc3BwTJkyIadOmxZ49e4qxHADQixQlPu6444644oorYs6cOXHqqafG8uXL43Of+1z8+te/LsZyAEAvUp73A/7nP/+JzZs3R0NDQ/e+fv36xdSpU+P5558/4PiOjo7o6Ojovt3a2hoREW1tbXmPFhERXR37ivK4ANBbFOPf2A8eM8uyjz029/h4++23o7OzM0aMGNFj/4gRI+KVV1454PjGxsZYvHjxAftHjx6d92gAQERUNhXvsd97772orKw85DG5x8fhamhoiPr6+u7bXV1d8e9//zuGDRsWZWVlua7V1tYWo0ePjpaWlhgyZEiuj30kcr59m/Pt24628404+s65r51vlmXx3nvvRVVV1ccem3t8fP7zn4/+/fvH7t27e+zfvXt3jBw58oDjC4VCFAqFHvuOPfbYvMfqYciQIX3iF/1JOd++zfn2bUfb+UYcfefcl8734654fCD3N5wOGDAgvvrVr8aaNWu693V1dcWaNWvi7LPPzns5AKCXKcrLLvX19VFbWxsTJ06MSZMmRVNTU7S3t8ecOXOKsRwA0IsUJT6++93vxr/+9a+44YYbYteuXXHaaafFU089dcCbUFMrFApx4403HvAyT1/lfPs259u3HW3nG3H0nfPRdr7/qyz7JJ+JAQDIie92AQCSEh8AQFLiAwBISnwAAEkdNfGxdOnSGDNmTAwcODDOOuus2LBhQ6lHKprGxsY488wzo6KiIoYPHx4XXHBBbN26tdRjJXHbbbdFWVlZzJs3r9SjFNWbb74Zl156aQwbNiwGDRoU48ePj02bNpV6rKLo7OyMRYsWRU1NTQwaNChOPPHEuOmmmz7R90f0BuvWrYuZM2dGVVVVlJWVxapVq3rcn2VZ3HDDDTFq1KgYNGhQTJ06NbZt21aaYXNwqPPdv39/LFiwIMaPHx+DBw+OqqqquOyyy+Ktt94q3cCf0cf9fv/XVVddFWVlZdHU1JRsvlI5KuLjkUceifr6+rjxxhujubk5JkyYENOmTYs9e/aUerSiePbZZ6Ouri7Wr18fq1evjv3798c3vvGNaG9vL/VoRbVx48a455574itf+UqpRymqd955JyZPnhzHHHNM/OEPf4i///3v8ctf/jKOO+64Uo9WFEuWLIlly5bF3XffHf/4xz9iyZIlcfvtt8ddd91V6tFy0d7eHhMmTIilS5ce9P7bb7897rzzzli+fHm88MILMXjw4Jg2bVq8//77iSfNx6HOd9++fdHc3ByLFi2K5ubmeOyxx2Lr1q3xrW99qwST5uPjfr8fWLlyZaxfv/4T/WnyPiE7CkyaNCmrq6vrvt3Z2ZlVVVVljY2NJZwqnT179mQRkT377LOlHqVo3nvvveykk07KVq9enX3961/Prr322lKPVDQLFizIzj333FKPkcyMGTOyyy+/vMe+73znO9msWbNKNFHxRES2cuXK7ttdXV3ZyJEjs5///Ofd+959992sUChkDz30UAkmzNeHz/dgNmzYkEVEtnPnzjRDFdFHne8///nP7Itf/GL28ssvZyeccEL2f//3f8lnS63PX/n4z3/+E5s3b46pU6d27+vXr19MnTo1nn/++RJOlk5ra2tERAwdOrTEkxRPXV1dzJgxo8fvua96/PHHY+LEiXHRRRfF8OHD4/TTT4/77ruv1GMVzTnnnBNr1qyJV199NSIiXnzxxXjuuedi+vTpJZ6s+Hbs2BG7du3q8b/rysrKOOuss46q56+ysrKif+dXqXR1dcXs2bNj/vz5MXbs2FKPk0zJv9W22N5+++3o7Ow84K+rjhgxIl555ZUSTZVOV1dXzJs3LyZPnhzjxo0r9ThF8fDDD0dzc3Ns3Lix1KMk8dprr8WyZcuivr4+rr/++ti4cWPMnTs3BgwYELW1taUeL3cLFy6Mtra2OOWUU6J///7R2dkZt9xyS8yaNavUoxXdrl27IiIO+vz1wX192fvvvx8LFiyI733ve33mi9c+bMmSJVFeXh5z584t9ShJ9fn4ONrV1dXFyy+/HM8991ypRymKlpaWuPbaa2P16tUxcODAUo+TRFdXV0ycODFuvfXWiIg4/fTT4+WXX47ly5f3yfh49NFH48EHH4wVK1bE2LFjY8uWLTFv3ryoqqrqk+fLf+3fvz8uvvjiyLIsli1bVupximLz5s3xq1/9Kpqbm6OsrKzU4yTV5192+fznPx/9+/eP3bt399i/e/fuGDlyZImmSuPqq6+OJ554ItauXRvHH398qccpis2bN8eePXvijDPOiPLy8igvL49nn3027rzzzigvL4/Ozs5Sj5i7UaNGxamnntpj35e//OV44403SjRRcc2fPz8WLlwYl1xySYwfPz5mz54d1113XTQ2NpZ6tKL74DnqaHv++iA8du7cGatXr+6zVz3+/Oc/x549e6K6urr7+Wvnzp3xwx/+MMaMGVPq8Yqqz8fHgAED4qtf/WqsWbOme19XV1esWbMmzj777BJOVjxZlsXVV18dK1eujD/96U9RU1NT6pGK5vzzz4+XXnoptmzZ0r1NnDgxZs2aFVu2bIn+/fuXesTcTZ48+YCPTr/66qtxwgknlGii4tq3b1/069fzqap///7R1dVVoonSqampiZEjR/Z4/mpra4sXXnihzz5/fRAe27Ztiz/+8Y8xbNiwUo9UNLNnz46//e1vPZ6/qqqqYv78+fH000+XeryiOipedqmvr4/a2tqYOHFiTJo0KZqamqK9vT3mzJlT6tGKoq6uLlasWBG/+93voqKiovu14crKyhg0aFCJp8tXRUXFAe9lGTx4cAwbNqzPvsfluuuui3POOSduvfXWuPjii2PDhg1x7733xr333lvq0Ypi5syZccstt0R1dXWMHTs2/vrXv8Ydd9wRl19+ealHy8XevXtj+/bt3bd37NgRW7ZsiaFDh0Z1dXXMmzcvbr755jjppJOipqYmFi1aFFVVVXHBBReUbujP4FDnO2rUqLjwwgujubk5nnjiiejs7Ox+/ho6dGgMGDCgVGN/ah/3+/1wXB1zzDExcuTIOPnkk1OPmlapP26Tyl133ZVVV1dnAwYMyCZNmpStX7++1CMVTUQcdPvNb35T6tGS6Osftc2yLPv973+fjRs3LisUCtkpp5yS3XvvvaUeqWja2tqya6+9Nquurs4GDhyYfelLX8p+/OMfZx0dHaUeLRdr16496P9fa2trsyz778dtFy1alI0YMSIrFArZ+eefn23durW0Q38GhzrfHTt2fOTz19q1a0s9+qfycb/fDztaPmpblmV95M8EAgC9Qp9/zwcAcGQRHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEn9Pys/hpPFiOhOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(quant_outputs['min_encoding_indices'].detach().cpu().numpy(), bins=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a115093-886a-4d0c-b311-ade11bb6cde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matplotlib.figure.Figure"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c03257-c1aa-4479-a185-dd8cbeee87ca",
   "metadata": {},
   "source": [
    "# Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0bcdb4e-5229-498b-a0f8-fbbee0ac0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourglassDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth=(4, 4),\n",
    "        elongate_factor = (2, 2),\n",
    "        upproj_factor = (2, 2),\n",
    "        attn_resampling = True,\n",
    "        updown_sample_type = 'linear',\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        causal = False,\n",
    "        norm_out = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # set up nesting\n",
    "        if isinstance(elongate_factor, (tuple, list, ListConfig)):\n",
    "            assert len(depth) == len(elongate_factor) == len(upproj_factor)\n",
    "            elongate_factor, *rest_elongate_factor = elongate_factor\n",
    "        elif isinstance(elongate_factor, int):\n",
    "            elongate_factor, rest_elongate_factor = elongate_factor, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        if isinstance(upproj_factor, (tuple, list, ListConfig)):\n",
    "            upproj_factor, *rest_upproj_factor = upproj_factor\n",
    "        elif isinstance(upproj_factor, int):\n",
    "            upproj_factor, rest_upproj_factor = upproj_factor, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        if isinstance(depth, (tuple, list, ListConfig)):\n",
    "            depth, *rest_depth = depth\n",
    "        elif isinstance(depth, int):\n",
    "            depth, rest_depth = depth, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        # shared transformer kwargs\n",
    "        transformer_kwargs = dict(\n",
    "            heads = heads,\n",
    "            dim_head = dim_head\n",
    "        )\n",
    "\n",
    "        self.causal = causal\n",
    "        self.elongate_factor = elongate_factor\n",
    "        self.upproj_factor = upproj_factor\n",
    "\n",
    "        if updown_sample_type == 'naive':\n",
    "            self.upsample = NaiveUpsample(elongate_factor)\n",
    "        elif updown_sample_type == 'linear':\n",
    "            self.upsample = LinearUpsample(dim, elongate_factor)\n",
    "        else:\n",
    "            raise ValueError(f'unknown updown_sample_type keyword value - must be either naive or linear for now')\n",
    "\n",
    "        self.up_projection = PreNormLinearUpProjection(dim, upproj_factor)\n",
    "        if _check_if_nest(rest_depth):\n",
    "            assert _check_if_nest(rest_elongate_factor)\n",
    "            assert _check_if_nest(rest_upproj_factor)\n",
    "            self.nested_decoder = HourglassDecoder(\n",
    "                dim = dim * upproj_factor,\n",
    "                elongate_factor = rest_elongate_factor,\n",
    "                upproj_factor = rest_upproj_factor,\n",
    "                depth = rest_depth,\n",
    "                attn_resampling = attn_resampling,\n",
    "                updown_sample_type = updown_sample_type,\n",
    "                causal = causal,\n",
    "                **transformer_kwargs\n",
    "            )\n",
    "            self.has_nest = True \n",
    "        else:\n",
    "            self.has_nest = False\n",
    "\n",
    "        self.post_transformer = Transformer(dim = dim * elongate_factor, depth = depth, causal = causal, **transformer_kwargs)\n",
    "        self.attn_resampling_post_valley = Transformer(dim = dim, depth = 1, **transformer_kwargs) if attn_resampling else None\n",
    "        self.norm_out = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, z_q, mask = None, original_length = None):\n",
    "        \"\"\"\n",
    "        z_q: input compressed representation\n",
    "        mask: indicates if input has a padding (True if we should keep it, False if it's padding & we should discard it)\n",
    "        compressed: at the start, should be None; if it's already populated, ignore further actions at the valley step\n",
    "        \"\"\"\n",
    "        # b : batch, n : sequence length, d : feature dimension, s : elongation factor\n",
    "        s, b, n = self.elongate_factor, *z_q.shape[:2]\n",
    "\n",
    "        # print(\"z_q\", z_q.shape)\n",
    "        upsampled = self.upsample(z_q)\n",
    "        # print(\"upsampled\", upsampled.shape)\n",
    "        # print(mask)\n",
    "        \n",
    "        # pad to multiple of shortening factor, in preparation for pooling\n",
    "        # z_q = pad_to_multiple(z_q, s, dim = -2)\n",
    "        # print('post pad maybe', z_q.shape)\n",
    "        # need to just ensure that input length is always a multiple of 2...\n",
    "\n",
    "        # naive repeat to upsample mask\n",
    "        if exists(mask):\n",
    "            # print(\"masks\", mask.shape)\n",
    "            # padded_mask = pad_to_multiple(mask, s, dim = -1, value = False)\n",
    "            # print(\"padded mask\", padded_mask)\n",
    "            upsampled_mask = einops.repeat(mask, 'b n -> b (n s)', s = s) > 0\n",
    "            # print(\"upsampled mask\", upsampled_mask)\n",
    "        else:\n",
    "            upsampled_mask = None\n",
    "        \n",
    "        # post-valley \"attention resampling\"\n",
    "        if exists(self.attn_resampling_post_valley):\n",
    "            x = self.attn_resampling_post_valley(\n",
    "                rearrange(upsampled, 'b (n s) d -> (b n) s d', s = s),\n",
    "                rearrange(z_q, 'b n d -> (b n) () d')\n",
    "            )\n",
    "            x = rearrange(x, '(b n) s d -> b (n s) d', b = b)\n",
    "        # print(\"x.shape\", x.shape)\n",
    "        x = self.up_projection(x)\n",
    "        # print(\"x.shape\", x.shape)\n",
    "\n",
    "        # bring sequence back to original length, if it were padded for pooling\n",
    "        if not original_length is None:\n",
    "            x = x[:, :original_length]\n",
    "        # print(x.shape)\n",
    "\n",
    "        # post-valley transformers\n",
    "        out = self.post_transformer(x, mask = upsampled_mask)\n",
    "        # print(out.shape)\n",
    "\n",
    "        if self.has_nest: \n",
    "            out = self.nested_decoder(out, mask = upsampled_mask)\n",
    "        \n",
    "        return self.norm_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "784905b0-e9b3-41a6-8b5a-b392399b965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = HourglassDecoder(\n",
    "    dim=1024 // downprojf,\n",
    "    depth=4,\n",
    "    elongate_factor=shortenf,\n",
    "    upproj_factor=downprojf,\n",
    "    attn_resampling=True,\n",
    "    updown_sample_type=\"linear\"\n",
    ")\n",
    "dec = dec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b6b492a-c932-4b99-b66c-f1ddca336dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_recons = dec(z_q, downsampled_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea546ce-3553-4661-984f-a6616e2abb13",
   "metadata": {},
   "source": [
    "#  Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a3556-f679-4a02-979b-1fb8531036a4",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b70ed9db-242e-4fbc-8608-84ced3c0658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plaid.datasets import CATHShardedDataModule\n",
    "\n",
    "shard_dir = \"/homefs/home/lux70/storage/data/cath/shards/\"\n",
    "dm = CATHShardedDataModule(\n",
    "    shard_dir=shard_dir\n",
    ")\n",
    "dm.setup()\n",
    "train_dataloader = dm.train_dataloader()\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0d48a2-65d8-481a-802f-280feb8ffe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plaid.esmfold.misc import batch_encode_sequences\n",
    "x = batch[0]\n",
    "_, mask, _, _, _ = batch_encode_sequences(batch[1])\n",
    "x = x.to(device)\n",
    "mask = mask.to(device).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59e6f6-1d4f-415e-9c8d-96642ab23c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df2793ea2c743aa9c7b782f48b4da13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 0:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0524786d889458bb4f234bbe7619141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87c7292e83545dc9140e2d01f12552b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 2:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d62f7b2b6b44b29ad18fae871a8e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 3:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a49df8f4c764a789d8dcfa5617530f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 4:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0dda7dd15e544658a334c65e603208e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 5:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc4547e5b1043a0b91bbfdfd3b788d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 6:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c390258b6842dc8f845fc578f7f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 7:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c18d449ce9b4c54bf39f2dfd2bae95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 8:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8df94cbee134dadb8d0eb4b507fab9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 9:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6dab5aba454299a3828b8400332cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 10:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09399345a484871b4557c433d5f98f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 11:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796e77be0d1b407dac6c69c3526fcad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 12:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c0c2552df040bab8bd1b08bcb27512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 13:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae40e5cca8e5432db9a8a8e940f7eca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 14:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e244dee0e64c47b117fdd13f7d4e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 15:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead3ef1ccbd64648b42a4ea63847c03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 16:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e382e1db1174ae9b66c09d9a194a936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 17:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd88077eb6c409ab481b5c5e7518e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 18:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef726e0a4ec4671a89bdb163b3d8c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 19:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da069b36229448ea8004f7c03a00b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 20:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6291747dc9c64622a11ca8d354bf76e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 21:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8283c3aa2d8d44dbac203a98d992ccd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 22:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(\n",
    "    list(enc.parameters()) + list(quantizer.parameters()) + list(dec.parameters()), lr=1e-4\n",
    ")\n",
    "\n",
    "# wandb.init(\n",
    "#     entity=\"lu-amy-al1\",\n",
    "#     project=\"plaid-hourglass-vqvae\",\n",
    "# )\n",
    "\n",
    "vq_losses = []\n",
    "perps = []\n",
    "recons_losses = []\n",
    "losses = []\n",
    "\n",
    "n_epochs=80\n",
    "for epoch in range(n_epochs): \n",
    "    for batch in tqdm(train_dataloader, desc=f\"epoch {epoch}\"):\n",
    "        ze, ze_mask = enc(x, mask)\n",
    "        quant_out = quantizer(ze)\n",
    "        zq = quant_out['z_q']\n",
    "        vq_loss = quant_out['loss']\n",
    "        # print(vq_loss)\n",
    "        # print(quant_out['perplexity'])\n",
    "        # print(quant_out)\n",
    "        x_recons = dec(zq, ze_mask) \n",
    "        \n",
    "        recons_loss = torch.mean((x_recons - x) ** 2)\n",
    "        loss = vq_loss + recons_loss\n",
    "        # print(loss)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        vq_losses.append(vq_loss.item())\n",
    "        perps.append(quant_out['perplexity'].item())\n",
    "        recons_losses.append(recons_loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # wandb.log({\n",
    "        #     \"vq_loss\": vq_loss,\n",
    "        #     \"vq_perplexity\": quant_out['perplexity'],\n",
    "        #     \"recons_loss\": recons_loss,\n",
    "        #     \"loss\": loss\n",
    "        # })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90703f53-cb88-42b6-9d0f-dd4fb083cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(np.min(recons_losses))\n",
    "plt.plot(recons_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69397e4c-6849-4f1a-ba65-6f06d38f18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), \"hourglass_vq_enc.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
