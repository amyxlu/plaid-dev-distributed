{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbac56de-4e4f-4065-9057-dca074a1f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e53f155-d9d7-42f9-9b44-fb00296a2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from omegaconf import ListConfig\n",
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866897ec-6b94-4b3f-bada-aaad9c83b0ec",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765904d5-50d6-40b5-8de3-736bad9cd515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as T\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, reduce, repeat\n",
    "from torch.optim import AdamW\n",
    "import lightning as L\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from omegaconf import ListConfig\n",
    "\n",
    "from plaid.datasets import CATHShardedDataModule\n",
    "from plaid.transforms import trim_or_pad_batch_first\n",
    "from plaid.esmfold.misc import batch_encode_sequences\n",
    "from plaid.utils import LatentScaler, get_lr_scheduler\n",
    "from plaid.proteins import LatentToSequence, LatentToStructure\n",
    "from plaid.losses.modules import SequenceAuxiliaryLoss, BackboneAuxiliaryLoss\n",
    "from plaid.losses.functions import masked_token_accuracy, masked_token_cross_entropy_loss\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def pad_to_multiple(tensor, multiple, dim = -1, value = 0):\n",
    "    seq_len = tensor.shape[dim]\n",
    "    m = seq_len / multiple\n",
    "    if m.is_integer():\n",
    "        return tensor\n",
    "    remainder = math.ceil(m) * multiple - seq_len\n",
    "    pad_offset = (0,) * (-1 - dim) * 2\n",
    "    return F.pad(tensor, (*pad_offset, 0, remainder), value = value)\n",
    "\n",
    "def cast_tuple(val, depth = 1):\n",
    "    return val if isinstance(val, tuple) else ((val,) * depth)\n",
    "\n",
    "# factory\n",
    "def _valid_depth_dtype(depth):\n",
    "    if isinstance(depth, int):\n",
    "        return True\n",
    "    if isinstance(depth, tuple) or isinstance(depth, list) or isinstance(depth, ListConfig):\n",
    "        if len(depth) == 3:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _check_if_nest(var):\n",
    "    return isinstance(var, (tuple, list, ListConfig)) and len(var) > 0\n",
    "\n",
    "# up and down sample classes\n",
    "\n",
    "class NaiveDownsample(nn.Module):\n",
    "    def __init__(self, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return reduce(x, 'b (n s) d -> b n d', 'mean', s = self.shorten_factor)\n",
    "\n",
    "class NaiveUpsample(nn.Module):\n",
    "    def __init__(self, elongate_factor):\n",
    "        super().__init__()\n",
    "        self.elongate_factor = elongate_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return repeat(x, 'b n d -> b (n s) d', s = self.elongate_factor)\n",
    "\n",
    "class LinearDownsample(nn.Module):\n",
    "    def __init__(self, dim, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim * shorten_factor, dim)\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b (n s) d -> b n (s d)', s = self.shorten_factor)\n",
    "        return self.proj(x)\n",
    "\n",
    "class LinearUpsample(nn.Module):\n",
    "    def __init__(self, dim, elongate_factor):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, dim * elongate_factor)\n",
    "        self.elongate_factor = elongate_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return rearrange(x, 'b n (s d) -> b (n s) d', s = self.elongate_factor)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNormLinearDownProjection(nn.Module):\n",
    "    def __init__(self, dim, downproj_factor):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.proj = nn.Linear(dim, dim // downproj_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(self.norm(x))\n",
    "\n",
    "class PreNormLinearUpProjection(nn.Module):\n",
    "    def __init__(self, dim, upproj_factor):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.proj = nn.Linear(dim, dim * upproj_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(self.norm(x))\n",
    "\n",
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs) + x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        causal = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = heads * dim_head\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h, device = self.heads, x.device\n",
    "        kv_input = default(context, x)\n",
    "\n",
    "        q, k, v = self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "        mask_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b j -> b () () j')\n",
    "            sim = sim.masked_fill(~mask, mask_value)\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = sim.shape[-2:]\n",
    "            mask = torch.ones(i, j, device = device, dtype = torch.bool).triu_(j - i + 1)\n",
    "            mask = rearrange(mask, 'i j -> () () i j')\n",
    "            sim = sim.masked_fill(mask, mask_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "def FeedForward(dim, mult = 4, dropout = 0.):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, dim * mult),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(dim * mult, dim)\n",
    "    )\n",
    "\n",
    "# transformer classes\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth,\n",
    "        causal = False,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_mult = 4,\n",
    "        ff_dropout = 0.,\n",
    "        norm_out = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNormResidual(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout, causal = causal)),\n",
    "                PreNormResidual(dim, FeedForward(dim, mult = ff_mult, dropout = ff_dropout))\n",
    "            ]))\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, compressed = None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, context = context, mask = mask)\n",
    "            x = ff(x)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80330cab-90a8-46f2-932f-4e3b8ba3f46c",
   "metadata": {},
   "source": [
    "# Hourglass Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff35672-8543-477d-a7dc-665541df2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourglassEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth=(4, 4),\n",
    "        shorten_factor = (2, 2),\n",
    "        downproj_factor = (2, 2),\n",
    "        attn_resampling = True,\n",
    "        updown_sample_type = 'naive',\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        causal = False,\n",
    "        norm_out = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # shorten factor\n",
    "        if isinstance(shorten_factor, (tuple, list, ListConfig)):\n",
    "            assert len(depth) == len(shorten_factor) == len(downproj_factor)\n",
    "            shorten_factor, *rest_shorten_factor = shorten_factor\n",
    "        elif isinstance(shorten_factor, int):\n",
    "            shorten_factor, rest_shorten_factor = shorten_factor, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        # downproj factor\n",
    "        if isinstance(downproj_factor, (tuple, list, ListConfig)):\n",
    "            downproj_factor, *rest_downproj_factor = downproj_factor\n",
    "        elif isinstance(downproj_factor, int):\n",
    "            downproj_factor, rest_downproj_factor = downproj_factor, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        # depth \n",
    "        if isinstance(depth, (tuple, list, ListConfig)):\n",
    "            depth, *rest_depth = depth\n",
    "        elif isinstance(depth, int):\n",
    "            depth, rest_depth = depth, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        # shared transformer kwargs\n",
    "        transformer_kwargs = dict(\n",
    "            heads = heads,\n",
    "            dim_head = dim_head\n",
    "        )\n",
    "\n",
    "        self.causal = causal\n",
    "        self.shorten_factor = shorten_factor\n",
    "        self.downproj_factor = downproj_factor\n",
    "\n",
    "        if updown_sample_type == 'naive':\n",
    "            self.downsample = NaiveDownsample(shorten_factor)\n",
    "        elif updown_sample_type == 'linear':\n",
    "            self.downsample = LinearDownsample(dim, shorten_factor)\n",
    "        else:\n",
    "            raise ValueError(f'unknown updown_sample_type keyword value - must be either naive or linear for now')\n",
    "\n",
    "        self.down_projection = PreNormLinearDownProjection(dim, downproj_factor)\n",
    "        if _check_if_nest(rest_depth):\n",
    "            assert _check_if_nest(rest_shorten_factor)\n",
    "            assert _check_if_nest(rest_downproj_factor)\n",
    "            self.nested_encoder = HourglassEncoder(\n",
    "                dim = dim // downproj_factor,\n",
    "                shorten_factor = rest_shorten_factor,\n",
    "                downproj_factor = rest_downproj_factor,\n",
    "                depth = rest_depth,\n",
    "                attn_resampling = attn_resampling,\n",
    "                updown_sample_type = updown_sample_type,\n",
    "                causal = causal,\n",
    "                **transformer_kwargs\n",
    "            )\n",
    "            self.has_nest = True \n",
    "        else:\n",
    "            self.has_nest = False\n",
    "\n",
    "        self.pre_transformer = Transformer(dim = dim, depth = depth, causal = causal, **transformer_kwargs)\n",
    "        self.attn_resampling_pre_valley = Transformer(dim = dim, depth = 1, **transformer_kwargs) if attn_resampling else None\n",
    "        self.norm_out = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        \"\"\"\n",
    "        x: input\n",
    "        mask: indicates if input has a padding (True if we should keep it, False if it's padding & we should discard it)\n",
    "        compressed: at the start, should be None; if it's already populated, ignore further actions at the valley step\n",
    "        \"\"\"\n",
    "        if x.shape[1] % self.shorten_factor != 0:\n",
    "            raise ValueError(\"Length of input `x` must be a multiple of `self.shorten_factor` for attention resampling\")\n",
    "\n",
    "        # b : batch, n : sequence length, d : feature dimension, s : shortening factor\n",
    "        s, b, n = self.shorten_factor, *x.shape[:2]\n",
    "\n",
    "        # top half of hourglass, pre-transformer layers\n",
    "        x = self.pre_transformer(x, mask = mask)\n",
    "\n",
    "        # pad to multiple of shortening factor, in preparation for pooling\n",
    "        x = pad_to_multiple(x, s, dim = -2)\n",
    "\n",
    "        # print(mask)\n",
    "        if exists(mask):\n",
    "            padded_mask = pad_to_multiple(mask, s, dim = -1, value = False)\n",
    "        # print(padded_mask)\n",
    "            \n",
    "\n",
    "        # save the residual, and for \"attention resampling\" at downsample and upsample\n",
    "        x_residual = x.clone()\n",
    "\n",
    "        # if autoregressive, do the shift by shortening factor minus one\n",
    "        if self.causal:\n",
    "            shift = s - 1\n",
    "            x = F.pad(x, (0, 0, shift, -shift), value = 0.)\n",
    "\n",
    "            if exists(mask):\n",
    "                padded_mask = F.pad(padded_mask, (shift, -shift), value = False)\n",
    "\n",
    "        # naive average pool along length dimension\n",
    "        downsampled = self.downsample(x)\n",
    "        if exists(mask):\n",
    "            downsampled_mask = reduce(padded_mask, 'b (n s) -> b n', 'sum', s = s) > 0\n",
    "        else:\n",
    "            downsampled_mask = None\n",
    "        # print('down sampled mask', downsampled_mask)\n",
    "\n",
    "        # pre-valley \"attention resampling\" - they have the pooled token in each bucket attend to the tokens pre-pooled\n",
    "        if exists(self.attn_resampling_pre_valley):\n",
    "            if exists(mask):\n",
    "                attn_resampling_mask = rearrange(padded_mask, 'b (n s) -> (b n) s', s = s)\n",
    "            else:\n",
    "                attn_resampling_mask = None\n",
    "            downsampled = self.attn_resampling_pre_valley(\n",
    "                rearrange(downsampled, 'b n d -> (b n) () d'),\n",
    "                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n",
    "                mask = attn_resampling_mask\n",
    "            )\n",
    "\n",
    "            downsampled = rearrange(downsampled, '(b n) () d -> b n d', b = b)\n",
    "            \n",
    "        # also possibly reduce along dim=-1\n",
    "        out = self.down_projection(downsampled)\n",
    "\n",
    "        # the \"valley\" - either a regular transformer or another hourglass\n",
    "        if self.has_nest: \n",
    "            out, downsampled_mask = self.nested_encoder(out, mask = downsampled_mask)\n",
    "        \n",
    "        return self.norm_out(out), downsampled_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f17d8d-5684-40f2-882e-dec7799339c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortenf = 2\n",
    "downprojf = 2\n",
    "enc = HourglassEncoder(\n",
    "    dim=1024,\n",
    "    depth=4,\n",
    "    shorten_factor=shortenf,\n",
    "    downproj_factor=downprojf,\n",
    "    attn_resampling=True,\n",
    "    updown_sample_type=\"linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1fb2e80-e3a7-4d71-9683-984b7ceb48a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "enc.to(device)\n",
    "emb = torch.randn((4,8,1024)).to(device)\n",
    "mask = einops.repeat(torch.tensor([1,1,1,1,1,0,0,0]), \"l -> n l\", n=4).bool().to(device)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3e6c4e-96cb-488a-91aa-3acebe828a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ze, downsampled_mask = enc(emb,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21431a4-288a-4bb4-ac1e-f6e0cd4b4617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 512])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(ze.shape)\n",
    "print(downsampled_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b0000c-14b3-4d22-a964-7c69fcf136bd",
   "metadata": {},
   "source": [
    "# Quantizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54063a2-f80e-4239-aa92-9ac84038e39e",
   "metadata": {},
   "source": [
    "## Vector-Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f8f0c1d-cbbe-4fbd-a552-a27af4c6b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def expand_to_shape(x, target_shape):\n",
    "    # keep adding dimensions to the end until we match target dimensions\n",
    "    while len(x.shape) < len(target_shape):\n",
    "        x = x[..., None]\n",
    "    return x.expand(target_shape)\n",
    "\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Discretization bottleneck part of the VQ-VAE.\n",
    "\n",
    "    Inputs:\n",
    "    - n_e : number of embeddings\n",
    "    - e_dim : dimension of embedding\n",
    "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, e_dim, beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.n_e = n_e\n",
    "        self.e_dim = e_dim\n",
    "        self.beta = beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Inputs the output of the encoder network z and maps it to a discrete \n",
    "        one-hot vector that is the index of the closest embedding vector e_j\n",
    "\n",
    "        z (continuous) -> z_q (discrete)\n",
    "\n",
    "        z.shape = (batch, length, channel)\n",
    "\n",
    "        quantization pipeline:\n",
    "\n",
    "            1. get encoder input (B,L,C)\n",
    "            2. flatten input to (B*L,C)\n",
    "\n",
    "        \"\"\"\n",
    "        # reshape z -> (batch, height, channel) and flatten\n",
    "        z_flattened = einops.rearrange(z, \"b l c -> (b l) c\").view(-1, self.e_dim)\n",
    "        device = z.device\n",
    "\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
    "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
    "            torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "\n",
    "        # find closest encodings\n",
    "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
    "        min_encodings = torch.zeros(\n",
    "            min_encoding_indices.shape[0], self.n_e).to(device)\n",
    "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
    "\n",
    "        # compute loss for embedding\n",
    "        # embedding_loss = masked_mse_loss(z_q.detach(), z, mask)\n",
    "        # commitment_loss = masked_mse_loss(z_q, z.detach(), mask)\n",
    "        # loss = embedding_loss + self.beta * commitment_loss\n",
    "        loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
    "            torch.mean((z_q - z.detach()) ** 2)\n",
    "\n",
    "        # preserve gradients\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # perplexity\n",
    "        e_mean = torch.mean(min_encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"z_q\": z_q,\n",
    "            \"perplexity\": perplexity,\n",
    "            \"min_encodings\": min_encodings,\n",
    "            \"min_encoding_indices\": min_encoding_indices\n",
    "        }\n",
    "\n",
    "    def get_codebook_entry(self, indices, shape=None):\n",
    "        # shape specifying (batch, length, channel)\n",
    "        # TODO: check for more easy handling with nn.Embedding\n",
    "        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n",
    "        min_encodings.scatter_(1, indices[:,None], 1)\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n",
    "\n",
    "        if shape is not None:\n",
    "            z_q = z_q.view(shape)\n",
    "        return z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29713cb9-5aeb-426e-a1db-6e7f060049b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_e = 16\n",
    "e_dim = 64\n",
    "quantizer = VectorQuantizer(n_e, e_dim, 0.25)\n",
    "quantizer = quantizer.to(device)\n",
    "quant_outputs = quantizer(ze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d1fea3f-4bc7-4914-b11b-949b94107509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss torch.Size([])\n",
      "z_q torch.Size([4, 4, 512])\n",
      "perplexity torch.Size([])\n",
      "min_encodings torch.Size([128, 16])\n",
      "min_encoding_indices torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "for k, v in quant_outputs.items():\n",
    "    print(k, v.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97e7ac93-700a-4fc8-955a-b7d474db1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_q = quant_outputs['z_q']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c03257-c1aa-4479-a185-dd8cbeee87ca",
   "metadata": {},
   "source": [
    "# Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0bcdb4e-5229-498b-a0f8-fbbee0ac0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourglassDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth=(4, 4),\n",
    "        elongate_factor = (2, 2),\n",
    "        upproj_factor = (2, 2),\n",
    "        attn_resampling = True,\n",
    "        updown_sample_type = 'linear',\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        causal = False,\n",
    "        norm_out = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # set up nesting\n",
    "        if isinstance(elongate_factor, (tuple, list, ListConfig)):\n",
    "            assert len(depth) == len(elongate_factor) == len(upproj_factor)\n",
    "            elongate_factor, *rest_elongate_factor = elongate_factor\n",
    "        elif isinstance(elongate_factor, int):\n",
    "            elongate_factor, rest_elongate_factor = elongate_factor, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        if isinstance(upproj_factor, (tuple, list, ListConfig)):\n",
    "            upproj_factor, *rest_upproj_factor = upproj_factor\n",
    "        elif isinstance(upproj_factor, int):\n",
    "            upproj_factor, rest_upproj_factor = upproj_factor, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        if isinstance(depth, (tuple, list, ListConfig)):\n",
    "            depth, *rest_depth = depth\n",
    "        elif isinstance(depth, int):\n",
    "            depth, rest_depth = depth, None\n",
    "        else:\n",
    "            raise TypeError()\n",
    "\n",
    "        # shared transformer kwargs\n",
    "        transformer_kwargs = dict(\n",
    "            heads = heads,\n",
    "            dim_head = dim_head\n",
    "        )\n",
    "\n",
    "        self.causal = causal\n",
    "        self.elongate_factor = elongate_factor\n",
    "        self.upproj_factor = upproj_factor\n",
    "\n",
    "        if updown_sample_type == 'naive':\n",
    "            self.upsample = NaiveUpsample(elongate_factor)\n",
    "        elif updown_sample_type == 'linear':\n",
    "            self.upsample = LinearUpsample(dim, elongate_factor)\n",
    "        else:\n",
    "            raise ValueError(f'unknown updown_sample_type keyword value - must be either naive or linear for now')\n",
    "\n",
    "        self.up_projection = PreNormLinearUpProjection(dim, upproj_factor)\n",
    "        if _check_if_nest(rest_depth):\n",
    "            assert _check_if_nest(rest_elongate_factor)\n",
    "            assert _check_if_nest(rest_upproj_factor)\n",
    "            self.nested_decoder = HourglassDecoder(\n",
    "                dim = dim * upproj_factor,\n",
    "                elongate_factor = rest_elongate_factor,\n",
    "                upproj_factor = rest_upproj_factor,\n",
    "                depth = rest_depth,\n",
    "                attn_resampling = attn_resampling,\n",
    "                updown_sample_type = updown_sample_type,\n",
    "                causal = causal,\n",
    "                **transformer_kwargs\n",
    "            )\n",
    "            self.has_nest = True \n",
    "        else:\n",
    "            self.has_nest = False\n",
    "\n",
    "        self.post_transformer = Transformer(dim = dim * elongate_factor, depth = depth, causal = causal, **transformer_kwargs)\n",
    "        self.attn_resampling_post_valley = Transformer(dim = dim, depth = 1, **transformer_kwargs) if attn_resampling else None\n",
    "        self.norm_out = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, z_q, mask = None, original_length = None):\n",
    "        \"\"\"\n",
    "        z_q: input compressed representation\n",
    "        mask: indicates if input has a padding (True if we should keep it, False if it's padding & we should discard it)\n",
    "        compressed: at the start, should be None; if it's already populated, ignore further actions at the valley step\n",
    "        \"\"\"\n",
    "        # b : batch, n : sequence length, d : feature dimension, s : elongation factor\n",
    "        s, b, n = self.elongate_factor, *z_q.shape[:2]\n",
    "\n",
    "        # print(\"z_q\", z_q.shape)\n",
    "        upsampled = self.upsample(z_q)\n",
    "        # print(\"upsampled\", upsampled.shape)\n",
    "        # print(mask)\n",
    "        \n",
    "        # pad to multiple of shortening factor, in preparation for pooling\n",
    "        # z_q = pad_to_multiple(z_q, s, dim = -2)\n",
    "        # print('post pad maybe', z_q.shape)\n",
    "        # need to just ensure that input length is always a multiple of 2...\n",
    "\n",
    "        # naive repeat to upsample mask\n",
    "        if exists(mask):\n",
    "            # print(\"masks\", mask.shape)\n",
    "            # padded_mask = pad_to_multiple(mask, s, dim = -1, value = False)\n",
    "            # print(\"padded mask\", padded_mask)\n",
    "            upsampled_mask = einops.repeat(mask, 'b n -> b (n s)', s = s) > 0\n",
    "            # print(\"upsampled mask\", upsampled_mask)\n",
    "        else:\n",
    "            upsampled_mask = None\n",
    "        \n",
    "        # post-valley \"attention resampling\"\n",
    "        if exists(self.attn_resampling_post_valley):\n",
    "            x = self.attn_resampling_post_valley(\n",
    "                rearrange(upsampled, 'b (n s) d -> (b n) s d', s = s),\n",
    "                rearrange(z_q, 'b n d -> (b n) () d')\n",
    "            )\n",
    "            x = rearrange(x, '(b n) s d -> b (n s) d', b = b)\n",
    "        # print(\"x.shape\", x.shape)\n",
    "        x = self.up_projection(x)\n",
    "        # print(\"x.shape\", x.shape)\n",
    "\n",
    "        # bring sequence back to original length, if it were padded for pooling\n",
    "        if not original_length is None:\n",
    "            x = x[:, :original_length]\n",
    "        # print(x.shape)\n",
    "\n",
    "        # post-valley transformers\n",
    "        out = self.post_transformer(x, mask = upsampled_mask)\n",
    "        # print(out.shape)\n",
    "\n",
    "        if self.has_nest: \n",
    "            out = self.nested_decoder(out, mask = upsampled_mask)\n",
    "        \n",
    "        return self.norm_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "784905b0-e9b3-41a6-8b5a-b392399b965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = HourglassDecoder(\n",
    "    dim=1024 // downprojf,\n",
    "    depth=4,\n",
    "    elongate_factor=shortenf,\n",
    "    upproj_factor=downprojf,\n",
    "    attn_resampling=True,\n",
    "    updown_sample_type=\"linear\"\n",
    ")\n",
    "dec = dec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b6b492a-c932-4b99-b66c-f1ddca336dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_recons = dec(z_q, downsampled_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea546ce-3553-4661-984f-a6616e2abb13",
   "metadata": {},
   "source": [
    "#  Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a3556-f679-4a02-979b-1fb8531036a4",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b70ed9db-242e-4fbc-8608-84ced3c0658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plaid.datasets import CATHShardedDataModule\n",
    "\n",
    "shard_dir = \"/homefs/home/lux70/storage/data/cath/shards/\"\n",
    "dm = CATHShardedDataModule(\n",
    "    shard_dir=shard_dir\n",
    ")\n",
    "dm.setup()\n",
    "train_dataloader = dm.train_dataloader()\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0d48a2-65d8-481a-802f-280feb8ffe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plaid.esmfold.misc import batch_encode_sequences\n",
    "x = batch[0]\n",
    "_, mask, _, _, _ = batch_encode_sequences(batch[1])\n",
    "x = x.to(device)\n",
    "mask = mask.to(device).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59e6f6-1d4f-415e-9c8d-96642ab23c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df2793ea2c743aa9c7b782f48b4da13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 0:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0524786d889458bb4f234bbe7619141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87c7292e83545dc9140e2d01f12552b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 2:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d62f7b2b6b44b29ad18fae871a8e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 3:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a49df8f4c764a789d8dcfa5617530f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 4:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0dda7dd15e544658a334c65e603208e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 5:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc4547e5b1043a0b91bbfdfd3b788d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 6:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c390258b6842dc8f845fc578f7f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 7:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c18d449ce9b4c54bf39f2dfd2bae95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 8:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8df94cbee134dadb8d0eb4b507fab9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 9:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6dab5aba454299a3828b8400332cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 10:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(\n",
    "    list(enc.parameters()) + list(quantizer.parameters()) + list(dec.parameters()), lr=1e-4\n",
    ")\n",
    "\n",
    "# wandb.init(\n",
    "#     entity=\"lu-amy-al1\",\n",
    "#     project=\"plaid-hourglass-vqvae\",\n",
    "# )\n",
    "\n",
    "vq_losses = []\n",
    "perps = []\n",
    "recons_losses = []\n",
    "losses = []\n",
    "\n",
    "n_epochs=80\n",
    "for epoch in range(n_epochs): \n",
    "    for batch in tqdm(train_dataloader, desc=f\"epoch {epoch}\"):\n",
    "        ze, ze_mask = enc(x, mask)\n",
    "        quant_out = quantizer(ze)\n",
    "        zq = quant_out['z_q']\n",
    "        vq_loss = quant_out['loss']\n",
    "        # print(vq_loss)\n",
    "        # print(quant_out['perplexity'])\n",
    "        # print(quant_out)\n",
    "        x_recons = dec(zq, ze_mask) \n",
    "        \n",
    "        recons_loss = torch.mean((x_recons - x) ** 2)\n",
    "        loss = vq_loss + recons_loss\n",
    "        # print(loss)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        vq_losses.append(vq_loss.item())\n",
    "        perps.append(quant_out['perplexity'].item())\n",
    "        recons_losses.append(recons_loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # wandb.log({\n",
    "        #     \"vq_loss\": vq_loss,\n",
    "        #     \"vq_perplexity\": quant_out['perplexity'],\n",
    "        #     \"recons_loss\": recons_loss,\n",
    "        #     \"loss\": loss\n",
    "        # })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90703f53-cb88-42b6-9d0f-dd4fb083cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(np.min(recons_losses))\n",
    "plt.plot(recons_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69397e4c-6849-4f1a-ba65-6f06d38f18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), \"hourglass_vq_enc.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
