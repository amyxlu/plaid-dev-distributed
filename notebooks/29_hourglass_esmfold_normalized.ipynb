{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069dd14d-c22d-4561-bd01-11c7e950dcb2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5650be11-405d-4851-b4d3-1513433239a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891c3eea-bcfb-4dfd-8cf5-0bb3a1ea5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from plaid.datasets import CATHShardedDataModule, CATHStructureDataModule\n",
    "from plaid.esmfold.misc import batch_encode_sequences\n",
    "from plaid.utils import LatentScaler\n",
    "from plaid.losses.modules import SequenceAuxiliaryLoss, BackboneAuxiliaryLoss\n",
    "from plaid.proteins import LatentToStructure, LatentToSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca9cb3-2120-4211-8abd-7300d04dada1",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "865afd3b-75f7-4687-bdb1-37a108cef252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shard_dir = \"/data/lux70/data/rocklin/shards/\"\n",
    "# pdb_dir = \"/data/lux70/data/rocklin/structures/\" \n",
    "shard_dir = \"/data/lux70/data/cath/shards/\"\n",
    "pdb_dir = \"/data/lux70/data/cath/dompdb/\"\n",
    "\n",
    "embedder = \"esmfold\"\n",
    "D = 1024 if embedder == \"esmfold\" else 320\n",
    "\n",
    "dm = CATHShardedDataModule(\n",
    "    storage_type=\"hdf5\",\n",
    "    shard_dir=shard_dir,\n",
    "    embedder=embedder,\n",
    "    seq_len=256,\n",
    "    batch_size=64\n",
    ")\n",
    "# dm = CATHStructureDataModule(\n",
    "#     shard_dir=shard_dir,\n",
    "#     pdb_path_dir=pdb_dir,\n",
    "#     embedder=embedder,\n",
    "#     seq_len=256,\n",
    "#     batch_size=16\n",
    "# )\n",
    "dm.setup()\n",
    "train_dataloader = dm.train_dataloader()\n",
    "val_dataloader = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264a5636-3d63-457b-ae8d-d91a8bf2d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_scaler = LatentScaler(lm_embedder_type=embedder)\n",
    "# sequence_constructor = LatentToSequence()\n",
    "# sequence_constructor.to(\"cuda\")\n",
    "# seq_loss = SequenceAuxiliaryLoss(sequence_constructor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13eeb53d-8520-4af3-b0fd-081f0dcdded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from plaid.losses.modules import BackboneAuxiliaryLoss\n",
    "# structure_constructor = LatentToStructure()\n",
    "# structure_constructor.to('cuda')\n",
    "# structure_loss_fn = BackboneAuxiliaryLoss(structure_constructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee1892-7dba-4758-867e-7f8e058933ab",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49545e87-569b-451a-b679-985ede0987ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bb7dda3-3423-4244-81e6-b5c135429efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def pad_to_multiple(tensor, multiple, dim = -1, value = 0):\n",
    "    seq_len = tensor.shape[dim]\n",
    "    m = seq_len / multiple\n",
    "    if m.is_integer():\n",
    "        return tensor\n",
    "    remainder = math.ceil(m) * multiple - seq_len\n",
    "    pad_offset = (0,) * (-1 - dim) * 2\n",
    "    return F.pad(tensor, (*pad_offset, 0, remainder), value = value)\n",
    "\n",
    "def cast_tuple(val, depth = 1):\n",
    "    return val if isinstance(val, tuple) else ((val,) * depth)\n",
    "\n",
    "# factory\n",
    "\n",
    "def get_hourglass_transformer(\n",
    "    dim,\n",
    "    *,\n",
    "    depth,\n",
    "    shorten_factor,\n",
    "    downproj_factor,\n",
    "    attn_resampling,\n",
    "    updown_sample_type,\n",
    "    **kwargs\n",
    "):\n",
    "    assert isinstance(depth, int) or (isinstance(depth, tuple)  and len(depth) == 3), 'depth must be either an integer or a tuple of 3, indicating (pre_transformer_depth, <nested-hour-glass-config>, post_transformer_depth)'\n",
    "    assert not (isinstance(depth, int) and shorten_factor), 'there does not need to be a shortening factor when only a single transformer block is indicated (depth of one integer value)'\n",
    "\n",
    "    if isinstance(depth, int):\n",
    "        return Transformer(dim = dim, depth = depth, **kwargs)\n",
    "\n",
    "    return HourglassTransformer(dim = dim, depth = depth, shorten_factor = shorten_factor, downproj_factor = downproj_factor, attn_resampling = attn_resampling, updown_sample_type = updown_sample_type, **kwargs)\n",
    "\n",
    "# up and down sample classes\n",
    "\n",
    "class NaiveDownsample(nn.Module):\n",
    "    def __init__(self, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return reduce(x, 'b (n s) d -> b n d', 'mean', s = self.shorten_factor)\n",
    "\n",
    "class NaiveUpsample(nn.Module):\n",
    "    def __init__(self, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return repeat(x, 'b n d -> b (n s) d', s = self.shorten_factor)\n",
    "\n",
    "class LinearDownsample(nn.Module):\n",
    "    def __init__(self, dim, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim * shorten_factor, dim)\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b (n s) d -> b n (s d)', s = self.shorten_factor)\n",
    "        return self.proj(x)\n",
    "\n",
    "class LinearUpsample(nn.Module):\n",
    "    def __init__(self, dim, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, dim * shorten_factor)\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return rearrange(x, 'b n (s d) -> b (n s) d', s = self.shorten_factor)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNormLinearDownProjection(nn.Module):\n",
    "    def __init__(self, dim, downproj_factor):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.proj = nn.Linear(dim, dim // downproj_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(self.norm(x))\n",
    "\n",
    "class PreNormLinearUpProjection(nn.Module):\n",
    "    def __init__(self, dim, downproj_factor):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim // downproj_factor)\n",
    "        self.proj = nn.Linear(dim // downproj_factor, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(self.norm(x))\n",
    "\n",
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs) + x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        causal = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = heads * dim_head\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h, device = self.heads, x.device\n",
    "        kv_input = default(context, x)\n",
    "\n",
    "        q, k, v = self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "        mask_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b j -> b () () j')\n",
    "            sim = sim.masked_fill(~mask, mask_value)\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = sim.shape[-2:]\n",
    "            mask = torch.ones(i, j, device = device, dtype = torch.bool).triu_(j - i + 1)\n",
    "            mask = rearrange(mask, 'i j -> () () i j')\n",
    "            sim = sim.masked_fill(mask, mask_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "def FeedForward(dim, mult = 4, dropout = 0.):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, dim * mult),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(dim * mult, dim)\n",
    "    )\n",
    "\n",
    "# transformer classes\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth,\n",
    "        causal = False,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_mult = 4,\n",
    "        ff_dropout = 0.,\n",
    "        norm_out = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNormResidual(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout, causal = causal)),\n",
    "                PreNormResidual(dim, FeedForward(dim, mult = ff_mult, dropout = ff_dropout))\n",
    "            ]))\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, context = context, mask = mask)\n",
    "            x = ff(x)\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class HourglassTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth,\n",
    "        shorten_factor = 2,\n",
    "        downproj_factor = 2,\n",
    "        attn_resampling = True,\n",
    "        updown_sample_type = 'naive',\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        causal = False,\n",
    "        norm_out = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        pre_layers_depth, valley_depth, post_layers_depth = depth\n",
    "\n",
    "        # shorten factor\n",
    "        if isinstance(shorten_factor, (tuple, list)):\n",
    "            shorten_factor, *rest_shorten_factor = shorten_factor\n",
    "        elif isinstance(valley_depth, int):\n",
    "            shorten_factor, rest_shorten_factor = shorten_factor, None\n",
    "        else:\n",
    "            shorten_factor, rest_shorten_factor = shorten_factor, shorten_factor\n",
    "\n",
    "        # downproj factor\n",
    "        if isinstance(downproj_factor, (tuple, list)):\n",
    "            downproj_factor, *rest_downproj_factor = downproj_factor\n",
    "        elif isinstance(valley_depth, int):\n",
    "            downproj_factor, rest_downproj_factor = downproj_factor, None\n",
    "        else:\n",
    "            downproj_factor, rest_downproj_factor = downproj_factor, downproj_factor\n",
    "\n",
    "        transformer_kwargs = dict(\n",
    "            heads = heads,\n",
    "            dim_head = dim_head\n",
    "        )\n",
    "\n",
    "        self.causal = causal\n",
    "        self.shorten_factor = shorten_factor\n",
    "        self.downproj_factor = downproj_factor\n",
    "\n",
    "        if updown_sample_type == 'naive':\n",
    "            self.downsample = NaiveDownsample(shorten_factor)\n",
    "            self.upsample   = NaiveUpsample(shorten_factor)\n",
    "        elif updown_sample_type == 'linear':\n",
    "            self.downsample = LinearDownsample(dim, shorten_factor)\n",
    "            self.upsample   = LinearUpsample(dim, shorten_factor)\n",
    "        else:\n",
    "            raise ValueError(f'unknown updown_sample_type keyword value - must be either naive or linear for now')\n",
    "\n",
    "        self.down_projection = PreNormLinearDownProjection(dim, downproj_factor)\n",
    "        self.up_projection = PreNormLinearUpProjection(dim, downproj_factor)\n",
    "        \n",
    "        self.valley_transformer = get_hourglass_transformer(\n",
    "            dim = dim // downproj_factor,\n",
    "            shorten_factor = rest_shorten_factor,\n",
    "            downproj_factor = rest_downproj_factor,\n",
    "            depth = valley_depth,\n",
    "            attn_resampling = attn_resampling,\n",
    "            updown_sample_type = updown_sample_type,\n",
    "            causal = causal,\n",
    "            **transformer_kwargs\n",
    "        )\n",
    "\n",
    "        self.attn_resampling_context_downproj = PreNormLinearDownProjection(dim, downproj_factor) if attn_resampling else None\n",
    "        self.attn_resampling_context_upproj = PreNormLinearUpProjection(dim, downproj_factor) if attn_resampling else None\n",
    "        self.attn_resampling_pre_valley = Transformer(dim = dim // downproj_factor, depth = 1, **transformer_kwargs) if attn_resampling else None\n",
    "        self.attn_resampling_post_valley = Transformer(dim = dim, depth = 1, **transformer_kwargs) if attn_resampling else None\n",
    "\n",
    "        self.pre_transformer = Transformer(dim = dim, depth = pre_layers_depth, causal = causal, **transformer_kwargs)\n",
    "        self.post_transformer = Transformer(dim = dim, depth = post_layers_depth, causal = causal, **transformer_kwargs)\n",
    "        self.norm_out = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # b : batch, n : sequence length, d : feature dimension, s : shortening factor\n",
    "        print('inp', x.shape)\n",
    "        s, b, n = self.shorten_factor, *x.shape[:2]\n",
    "        \n",
    "        # top half of hourglass, pre-transformer layers\n",
    "        x = self.pre_transformer(x, mask = mask)\n",
    "        print('after pre transformer', x.shape)\n",
    "        \n",
    "        # pad to multiple of shortening factor, in preparation for pooling\n",
    "        x = pad_to_multiple(x, s, dim = -2)\n",
    "\n",
    "        if exists(mask):\n",
    "            padded_mask = pad_to_multiple(mask, s, dim = -1, value = False)\n",
    "        print('after pad to multiple', x.shape)\n",
    "\n",
    "        # save the residual, and for \"attention resampling\" at downsample and upsample\n",
    "        x_residual = x.clone()\n",
    "\n",
    "        # if autoregressive, do the shift by shortening factor minus one\n",
    "        if self.causal:\n",
    "            shift = s - 1\n",
    "            x = F.pad(x, (0, 0, shift, -shift), value = 0.)\n",
    "\n",
    "            if exists(mask):\n",
    "                padded_mask = F.pad(padded_mask, (shift, -shift), value = False)\n",
    "        print('after potentially short', x.shape)\n",
    "\n",
    "        # naive average pool\n",
    "        downsampled = self.downsample(x)\n",
    "        print('after downsample', downsampled.shape)\n",
    "        if exists(mask):\n",
    "            downsampled_mask = reduce(padded_mask, 'b (n s) -> b n', 'sum', s = s) > 0\n",
    "        else:\n",
    "            downsampled_mask = None\n",
    "\n",
    "        # also possibly reduce along dim=-1\n",
    "        downsampled = self.down_projection(downsampled)\n",
    "        print('after downproj', downsampled.shape)\n",
    "\n",
    "        # pre-valley \"attention resampling\" - they have the pooled token in each bucket attend to the tokens pre-pooled\n",
    "        if exists(self.attn_resampling_pre_valley):\n",
    "            if exists(mask):\n",
    "                attn_resampling_mask = rearrange(padded_mask, 'b (n s) -> (b n) s', s = s)\n",
    "            else:\n",
    "                attn_resampling_mask = None\n",
    "            print('after pre valley attention resampling', x.shape)\n",
    "            downsampled = self.attn_resampling_pre_valley(\n",
    "                rearrange(downsampled, 'b n d -> (b n) () d'),\n",
    "                rearrange(self.attn_resampling_context_downproj(x), 'b (n s) d -> (b n) s d', s = s),\n",
    "                mask = attn_resampling_mask\n",
    "            )\n",
    "\n",
    "            downsampled = rearrange(downsampled, '(b n) () d -> b n d', b = b)\n",
    "            print('after pre valley resampling rearrange', downsampled.shape)\n",
    "            \n",
    "        # the \"valley\" - either a regular transformer or another hourglass\n",
    "        x = self.valley_transformer(downsampled, mask = downsampled_mask)\n",
    "        print('after valley output', x.shape)\n",
    "        valley_out = x.clone()\n",
    "\n",
    "        # naive repeat upsample\n",
    "        x = self.upsample(x)\n",
    "        print('after upsample', x.shape)\n",
    "        x = self.up_projection(x)\n",
    "        print('after up proj', x.shape)\n",
    "        \n",
    "        # add the residual\n",
    "        print(\"x_residual shape\", x_residual.shape)\n",
    "        x = x + x_residual\n",
    "        \n",
    "        # post-valley \"attention resampling\"\n",
    "        if exists(self.attn_resampling_post_valley):\n",
    "            x = self.attn_resampling_post_valley(\n",
    "                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n",
    "                rearrange(self.attn_resampling_context_upproj(valley_out), 'b n d -> (b n) () d')\n",
    "            )\n",
    "\n",
    "            x = rearrange(x, '(b n) s d -> b (n s) d', b = b)\n",
    "\n",
    "        print('after post valley attention resampling', x.shape)\n",
    "\n",
    "        # bring sequence back to original length, if it were padded for pooling\n",
    "        x = x[:, :n]\n",
    "        print('after maybe remove pad', x.shape)\n",
    "        \n",
    "        # post-valley transformers\n",
    "        x = self.post_transformer(x, mask = mask)\n",
    "        print('after post valley transformer', x.shape)\n",
    "        return self.norm_out(x)\n",
    "\n",
    "transformer = get_hourglass_transformer(\n",
    "    dim = D,                     # feature dimension\n",
    "    heads = 8,                      # attention heads\n",
    "    dim_head = 64,                  # dimension per attention head\n",
    "    shorten_factor = 2,             # shortening factor\n",
    "    downproj_factor = 2,\n",
    "    depth = (4, 2, 4),              # tuple of 3, standing for pre-transformer-layers, valley-transformer-layers (after downsample), post-transformer-layers (after upsample) - the valley transformer layers can be yet another nested tuple, in which case it will shorten again recursively\n",
    "    attn_resampling = True,\n",
    "    updown_sample_type = \"naive\",\n",
    "    causal = True,\n",
    "    norm_out = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed71e7-c59e-4b32-8016-97a94ef91d92",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35eaff9-b00d-4e9d-9284-cbe7a2f8168e",
   "metadata": {},
   "source": [
    "## Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1655ed6-be27-430e-8827-e141e3ddef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 1024])\n",
      "torch.Size([64, 256])\n",
      "tensor(1.3661, device='cuda:0') tensor(72.3197, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0') tensor(0.2281, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "sequences = batch[1]\n",
    "tokens, mask, _, _, _ = batch_encode_sequences(sequences)\n",
    "\n",
    "x = batch[0]\n",
    "if embedder != \"esmfold\":\n",
    "    x = x[:, 1:-1, :]\n",
    "mask = mask.bool()\n",
    "print(x.shape)\n",
    "print(mask.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a32f8ce-3ea1-4a15-bb15-5b90e127ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0034, device='cuda:0') tensor(0.1472, device='cuda:0')\n",
      "tensor(0.0034, device='cuda:0') tensor(0.1472, device='cuda:0')\n",
      "inp torch.Size([64, 256, 1024])\n",
      "after pre transformer torch.Size([64, 256, 1024])\n",
      "after pad to multiple torch.Size([64, 256, 1024])\n",
      "after potentially short torch.Size([64, 256, 1024])\n",
      "after downsample torch.Size([64, 128, 1024])\n",
      "after downproj torch.Size([64, 128, 512])\n",
      "after pre valley attention resampling torch.Size([64, 256, 1024])\n",
      "after pre valley resampling rearrange torch.Size([64, 128, 512])\n",
      "inp torch.Size([64, 128, 512])\n",
      "after pre transformer torch.Size([64, 128, 512])\n",
      "after pad to multiple torch.Size([64, 128, 512])\n",
      "after potentially short torch.Size([64, 128, 512])\n",
      "after downsample torch.Size([64, 64, 512])\n",
      "after downproj torch.Size([64, 64, 256])\n",
      "after pre valley attention resampling torch.Size([64, 128, 512])\n",
      "after pre valley resampling rearrange torch.Size([64, 64, 256])\n",
      "after valley output torch.Size([64, 64, 256])\n",
      "after upsample torch.Size([64, 128, 256])\n",
      "after up proj torch.Size([64, 128, 512])\n",
      "x_residual shape torch.Size([64, 128, 512])\n",
      "after post valley attention resampling torch.Size([64, 128, 512])\n",
      "after maybe remove pad torch.Size([64, 128, 512])\n",
      "after post valley transformer torch.Size([64, 128, 512])\n",
      "after valley output torch.Size([64, 128, 512])\n",
      "after upsample torch.Size([64, 256, 512])\n",
      "after up proj torch.Size([64, 256, 1024])\n",
      "x_residual shape torch.Size([64, 256, 1024])\n",
      "after post valley attention resampling torch.Size([64, 256, 1024])\n",
      "after maybe remove pad torch.Size([64, 256, 1024])\n",
      "after post valley transformer torch.Size([64, 256, 1024])\n"
     ]
    }
   ],
   "source": [
    "transformer = get_hourglass_transformer(\n",
    "    dim = D,                     # feature dimension\n",
    "    heads = 8,                      # attention heads\n",
    "    dim_head = 64,                  # dimension per attention head\n",
    "    shorten_factor = (2, 2),             # shortening factor\n",
    "    downproj_factor = (2, 2),\n",
    "    depth = (4, (4, 2, 4), 4),              # tuple of 3, standing for pre-transformer-layers, valley-transformer-layers (after downsample), post-transformer-layers (after upsample) - the valley transformer layers can be yet another nested tuple, in which case it will shorten again recursively\n",
    "    attn_resampling = True,\n",
    "    updown_sample_type = \"naive\",\n",
    "    causal = True,\n",
    "    norm_out = True\n",
    ")\n",
    "\n",
    "device = \"cuda\"\n",
    "transformer = transformer.to(device)\n",
    "x, mask = x.to(device), mask.to(device)\n",
    "print(x.mean(), x.std())\n",
    "\n",
    "x = latent_scaler.scale(x)\n",
    "print(x.mean(), x.std())\n",
    "\n",
    "output = transformer(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9a5e5b8-4581-4f57-b578-587981fa2345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating structure from latents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.9506, device='cuda:0'), {'backbone_loss': 0.9506101012229919})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# structure_loss_fn(output, batch[-1], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503dcfa-4525-4a5f-bce8-372b8293c623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0fb6d2d-6c4a-4e8c-9240-ef5d63f7c019",
   "metadata": {},
   "source": [
    "### Reconstruct Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7ac53-04f8-45a9-9881-8812c04f8cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13142d3-3d8b-4945-966e-d3ea7cd08bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_x_recons = latent_scaler.unscale(output)\n",
    "sequence_constructor = LatentToSequence()\n",
    "probs, _, strs = sequence_constructor.to_sequence(unnormalized_x_recons, mask, return_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e7f7d-6ff0-41be-8a15-f2947963d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unnormalized_x_recons.mean(), unnormalized_x_recons.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb62862-42f4-426f-b8f6-c2a8edc405f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "strs[:10]  # random strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3317f-7910-4f08-9ade-87d1aa08be4f",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22956a45-e3ee-449f-bef0-30467a2d610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(transformer.parameters(), lr=1e-4)\n",
    "n_epochs = 100\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in trange(n_epochs): \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        sequences = batch[1]\n",
    "        tokens, mask, _, _, _ = batch_encode_sequences(sequences)\n",
    "\n",
    "        x = batch[0]\n",
    "        if embedder != \"esmfold\":\n",
    "            x = x[:, 1:-1, :]\n",
    "        else:\n",
    "            x = latent_scaler.scale(x)\n",
    "        mask = mask.bool()\n",
    "        x, mask = x.to(device), mask.to(device)\n",
    "        \n",
    "        output = transformer(x, mask)\n",
    "        recons_loss = F.mse_loss(x, output)\n",
    "        if i % 50 == 0:\n",
    "            print(loss.item()) \n",
    "\n",
    "        unnormalized_x_recons = latent_scaler.unscale(output)\n",
    "        \n",
    "        logits, _, strs = sequence_constructor.to_sequence(unnormalized_x_recons, mask, return_logits=True)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed73124-df33-40d0-9091-14d34c66bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56d600-c7a2-4d2a-9209-dbd385cd5a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
