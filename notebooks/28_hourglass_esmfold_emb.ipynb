{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069dd14d-c22d-4561-bd01-11c7e950dcb2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5650be11-405d-4851-b4d3-1513433239a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "891c3eea-bcfb-4dfd-8cf5-0bb3a1ea5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from plaid.datasets import CATHShardedDataModule\n",
    "from plaid.esmfold.misc import batch_encode_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca9cb3-2120-4211-8abd-7300d04dada1",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "865afd3b-75f7-4687-bdb1-37a108cef252",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_dir = \"/homefs/home/lux70/storage/data/rocklin/shards/\"\n",
    "# embedder = \"esmfold\"\n",
    "embedder = \"esm2_t6_8M_UR50D\"\n",
    "D = 1024 if embedder == \"esmfold\" else 320\n",
    "\n",
    "dm = CATHShardedDataModule(\n",
    "    storage_type=\"hdf5\",\n",
    "    shard_dir=shard_dir,\n",
    "    embedder=embedder,\n",
    "    seq_len=256,\n",
    "    batch_size=512\n",
    ")\n",
    "dm.setup()\n",
    "train_dataloader = dm.train_dataloader()\n",
    "val_dataloader = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee1892-7dba-4758-867e-7f8e058933ab",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "22c1557a-7e57-4d9f-94da-153815624c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def pad_to_multiple(tensor, multiple, dim = -1, value = 0):\n",
    "    seq_len = tensor.shape[dim]\n",
    "    m = seq_len / multiple\n",
    "    if m.is_integer():\n",
    "        return tensor\n",
    "    remainder = math.ceil(m) * multiple - seq_len\n",
    "    pad_offset = (0,) * (-1 - dim) * 2\n",
    "    return F.pad(tensor, (*pad_offset, 0, remainder), value = value)\n",
    "\n",
    "def cast_tuple(val, depth = 1):\n",
    "    return val if isinstance(val, tuple) else ((val,) * depth)\n",
    "\n",
    "# factory\n",
    "\n",
    "def get_hourglass_transformer(\n",
    "    dim,\n",
    "    *,\n",
    "    depth,\n",
    "    shorten_factor,\n",
    "    attn_resampling,\n",
    "    updown_sample_type,\n",
    "    **kwargs\n",
    "):\n",
    "    assert isinstance(depth, int) or (isinstance(depth, tuple)  and len(depth) == 3), 'depth must be either an integer or a tuple of 3, indicating (pre_transformer_depth, <nested-hour-glass-config>, post_transformer_depth)'\n",
    "    assert not (isinstance(depth, int) and shorten_factor), 'there does not need to be a shortening factor when only a single transformer block is indicated (depth of one integer value)'\n",
    "\n",
    "    if isinstance(depth, int):\n",
    "        return Transformer(dim = dim, depth = depth, **kwargs)\n",
    "\n",
    "    return HourglassTransformer(dim = dim, depth = depth, shorten_factor = shorten_factor, attn_resampling = attn_resampling, updown_sample_type = updown_sample_type, **kwargs)\n",
    "\n",
    "# up and down sample classes\n",
    "\n",
    "class NaiveDownsample(nn.Module):\n",
    "    def __init__(self, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return reduce(x, 'b (n s) d -> b n d', 'mean', s = self.shorten_factor)\n",
    "\n",
    "class NaiveUpsample(nn.Module):\n",
    "    def __init__(self, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return repeat(x, 'b n d -> b (n s) d', s = self.shorten_factor)\n",
    "\n",
    "class LinearDownsample(nn.Module):\n",
    "    def __init__(self, dim, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim * shorten_factor, dim)\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b (n s) d -> b n (s d)', s = self.shorten_factor)\n",
    "        return self.proj(x)\n",
    "\n",
    "class LinearUpsample(nn.Module):\n",
    "    def __init__(self, dim, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, dim * shorten_factor)\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return rearrange(x, 'b n (s d) -> b (n s) d', s = self.shorten_factor)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs) + x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        causal = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = heads * dim_head\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h, device = self.heads, x.device\n",
    "        kv_input = default(context, x)\n",
    "\n",
    "        q, k, v = self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "        mask_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b j -> b () () j')\n",
    "            sim = sim.masked_fill(~mask, mask_value)\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = sim.shape[-2:]\n",
    "            mask = torch.ones(i, j, device = device, dtype = torch.bool).triu_(j - i + 1)\n",
    "            mask = rearrange(mask, 'i j -> () () i j')\n",
    "            sim = sim.masked_fill(mask, mask_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "def FeedForward(dim, mult = 4, dropout = 0.):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, dim * mult),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(dim * mult, dim)\n",
    "    )\n",
    "\n",
    "# transformer classes\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth,\n",
    "        causal = False,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_mult = 4,\n",
    "        ff_dropout = 0.,\n",
    "        norm_out = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNormResidual(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout, causal = causal)),\n",
    "                PreNormResidual(dim, FeedForward(dim, mult = ff_mult, dropout = ff_dropout))\n",
    "            ]))\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, context = context, mask = mask)\n",
    "            x = ff(x)\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class HourglassTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth,\n",
    "        shorten_factor = 2,\n",
    "        attn_resampling = True,\n",
    "        updown_sample_type = 'naive',\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        causal = False,\n",
    "        norm_out = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(depth) == 3, 'depth should be a tuple of length 3'\n",
    "        assert updown_sample_type in {'naive', 'linear'}, 'downsample / upsample type must be either naive (average pool and repeat) or linear (linear projection and reshape)'\n",
    "\n",
    "        pre_layers_depth, valley_depth, post_layers_depth = depth\n",
    "\n",
    "        if isinstance(shorten_factor, (tuple, list)):\n",
    "            shorten_factor, *rest_shorten_factor = shorten_factor\n",
    "        elif isinstance(valley_depth, int):\n",
    "            shorten_factor, rest_shorten_factor = shorten_factor, None\n",
    "        else:\n",
    "            shorten_factor, rest_shorten_factor = shorten_factor, shorten_factor\n",
    "\n",
    "        transformer_kwargs = dict(\n",
    "            dim = dim,\n",
    "            heads = heads,\n",
    "            dim_head = dim_head\n",
    "        )\n",
    "\n",
    "        self.causal = causal\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "        if updown_sample_type == 'naive':\n",
    "            self.downsample = NaiveDownsample(shorten_factor)\n",
    "            self.upsample   = NaiveUpsample(shorten_factor)\n",
    "        elif updown_sample_type == 'linear':\n",
    "            self.downsample = LinearDownsample(dim, shorten_factor)\n",
    "            self.upsample   = LinearUpsample(dim, shorten_factor)\n",
    "        else:\n",
    "            raise ValueError(f'unknown updown_sample_type keyword value - must be either naive or linear for now')\n",
    "\n",
    "        self.valley_transformer = get_hourglass_transformer(\n",
    "            shorten_factor = rest_shorten_factor,\n",
    "            depth = valley_depth,\n",
    "            attn_resampling = attn_resampling,\n",
    "            updown_sample_type = updown_sample_type,\n",
    "            causal = causal,\n",
    "            **transformer_kwargs\n",
    "        )\n",
    "\n",
    "        self.attn_resampling_pre_valley = Transformer(depth = 1, **transformer_kwargs) if attn_resampling else None\n",
    "        self.attn_resampling_post_valley = Transformer(depth = 1, **transformer_kwargs) if attn_resampling else None\n",
    "\n",
    "        self.pre_transformer = Transformer(depth = pre_layers_depth, causal = causal, **transformer_kwargs)\n",
    "        self.post_transformer = Transformer(depth = post_layers_depth, causal = causal, **transformer_kwargs)\n",
    "        self.norm_out = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # b : batch, n : sequence length, d : feature dimension, s : shortening factor\n",
    "\n",
    "        s, b, n = self.shorten_factor, *x.shape[:2]\n",
    "\n",
    "        # top half of hourglass, pre-transformer layers\n",
    "\n",
    "        x = self.pre_transformer(x, mask = mask)\n",
    "\n",
    "        # pad to multiple of shortening factor, in preparation for pooling\n",
    "\n",
    "        x = pad_to_multiple(x, s, dim = -2)\n",
    "\n",
    "        if exists(mask):\n",
    "            padded_mask = pad_to_multiple(mask, s, dim = -1, value = False)\n",
    "\n",
    "        # save the residual, and for \"attention resampling\" at downsample and upsample\n",
    "\n",
    "        x_residual = x.clone()\n",
    "\n",
    "        # if autoregressive, do the shift by shortening factor minus one\n",
    "\n",
    "        if self.causal:\n",
    "            shift = s - 1\n",
    "            x = F.pad(x, (0, 0, shift, -shift), value = 0.)\n",
    "\n",
    "            if exists(mask):\n",
    "                padded_mask = F.pad(padded_mask, (shift, -shift), value = False)\n",
    "\n",
    "        # naive average pool\n",
    "\n",
    "        downsampled = self.downsample(x)\n",
    "\n",
    "        if exists(mask):\n",
    "            downsampled_mask = reduce(padded_mask, 'b (n s) -> b n', 'sum', s = s) > 0\n",
    "        else:\n",
    "            downsampled_mask = None\n",
    "\n",
    "        # pre-valley \"attention resampling\" - they have the pooled token in each bucket attend to the tokens pre-pooled\n",
    "\n",
    "        if exists(self.attn_resampling_pre_valley):\n",
    "            if exists(mask):\n",
    "                attn_resampling_mask = rearrange(padded_mask, 'b (n s) -> (b n) s', s = s)\n",
    "            else:\n",
    "                attn_resampling_mask = None\n",
    "\n",
    "            downsampled = self.attn_resampling_pre_valley(\n",
    "                rearrange(downsampled, 'b n d -> (b n) () d'),\n",
    "                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n",
    "                mask = attn_resampling_mask\n",
    "            )\n",
    "\n",
    "            downsampled = rearrange(downsampled, '(b n) () d -> b n d', b = b)\n",
    "\n",
    "        # the \"valley\" - either a regular transformer or another hourglass\n",
    "\n",
    "        x = self.valley_transformer(downsampled, mask = downsampled_mask)\n",
    "\n",
    "        valley_out = x.clone()\n",
    "\n",
    "        # naive repeat upsample\n",
    "\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        # add the residual\n",
    "\n",
    "        x = x + x_residual\n",
    "\n",
    "        # post-valley \"attention resampling\"\n",
    "\n",
    "        if exists(self.attn_resampling_post_valley):\n",
    "            x = self.attn_resampling_post_valley(\n",
    "                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n",
    "                rearrange(valley_out, 'b n d -> (b n) () d')\n",
    "            )\n",
    "\n",
    "            x = rearrange(x, '(b n) s d -> b (n s) d', b = b)\n",
    "\n",
    "        # bring sequence back to original length, if it were padded for pooling\n",
    "\n",
    "        x = x[:, :n]\n",
    "\n",
    "        # post-valley transformers\n",
    "\n",
    "        x = self.post_transformer(x, mask = mask)\n",
    "        return self.norm_out(x)\n",
    "\n",
    "transformer = get_hourglass_transformer(\n",
    "    dim = D,                     # feature dimension\n",
    "    heads = 8,                      # attention heads\n",
    "    dim_head = 64,                  # dimension per attention head\n",
    "    shorten_factor = 2,             # shortening factor\n",
    "    depth = (4, 2, 4),              # tuple of 3, standing for pre-transformer-layers, valley-transformer-layers (after downsample), post-transformer-layers (after upsample) - the valley transformer layers can be yet another nested tuple, in which case it will shorten again recursively\n",
    "    attn_resampling = True,\n",
    "    updown_sample_type = \"naive\",\n",
    "    causal = True,\n",
    "    norm_out = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed71e7-c59e-4b32-8016-97a94ef91d92",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35eaff9-b00d-4e9d-9284-cbe7a2f8168e",
   "metadata": {},
   "source": [
    "## Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1655ed6-be27-430e-8827-e141e3ddef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 43, 320])\n",
      "torch.Size([512, 43])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "sequences = batch[1]\n",
    "tokens, mask, _, _, _ = batch_encode_sequences(sequences)\n",
    "\n",
    "x = batch[0]\n",
    "if embedder != \"esmfold\":\n",
    "    x = x[:, 1:-1, :]\n",
    "mask = mask.bool()\n",
    "print(x.shape)\n",
    "print(mask.shape)\n",
    "\n",
    "device = \"cuda\"\n",
    "transformer = transformer.to(device)\n",
    "x, mask = x.to(device), mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a32f8ce-3ea1-4a15-bb15-5b90e127ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "213b5dac-3e7f-48e6-9d30-269c2cb49bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 43, 320])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3317f-7910-4f08-9ade-87d1aa08be4f",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22956a45-e3ee-449f-bef0-30467a2d610e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2a83ab3ed54276aa3323d5536097e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035845961421728134\n",
      "0.033326685428619385\n",
      "0.031069863587617874\n",
      "0.029672497883439064\n",
      "0.028546307235956192\n",
      "0.02752477489411831\n",
      "0.026572855189442635\n",
      "0.025748198851943016\n",
      "0.02485238015651703\n",
      "0.024377355352044106\n",
      "0.023246990516781807\n",
      "0.022477811202406883\n",
      "0.021738136187195778\n",
      "0.02120210975408554\n",
      "0.02036917395889759\n",
      "0.019934097304940224\n",
      "0.0191204734146595\n",
      "0.01850868947803974\n",
      "0.01789453998208046\n",
      "0.017309917137026787\n",
      "0.01691448874771595\n",
      "0.016384942457079887\n",
      "0.015899430960416794\n",
      "0.01532022561877966\n",
      "0.014860481023788452\n",
      "0.014414000324904919\n",
      "0.014095748774707317\n",
      "0.013452598825097084\n",
      "0.013421254232525826\n",
      "0.012859363108873367\n",
      "0.01231471449136734\n",
      "0.011976531706750393\n",
      "0.011537332087755203\n",
      "0.011355330236256123\n",
      "0.010985419154167175\n",
      "0.010843458585441113\n",
      "0.010638715699315071\n",
      "0.010002760216593742\n",
      "0.009749649092555046\n",
      "0.009461858309805393\n",
      "0.009264327585697174\n",
      "0.009261995553970337\n",
      "0.008769209496676922\n",
      "0.00890357792377472\n",
      "0.008274618536233902\n",
      "0.00820575188845396\n",
      "0.00798237044364214\n",
      "0.007679465226829052\n",
      "0.0075495135970413685\n",
      "0.007317217066884041\n",
      "0.007275159936398268\n",
      "0.007045974489301443\n",
      "0.007205917499959469\n",
      "0.006788595113903284\n",
      "0.006644203327596188\n",
      "0.006482924800366163\n",
      "0.0064432621002197266\n",
      "0.006258669309318066\n",
      "0.0060765123926103115\n",
      "0.006106960587203503\n",
      "0.0059916917234659195\n",
      "0.005692700389772654\n",
      "0.005686743184924126\n",
      "0.005502649582922459\n",
      "0.00551061425358057\n",
      "0.005395092070102692\n",
      "0.005325683392584324\n",
      "0.005164287984371185\n",
      "0.005069877486675978\n",
      "0.005049072206020355\n",
      "0.005216455087065697\n",
      "0.004911255091428757\n",
      "0.004846156109124422\n",
      "0.004838914610445499\n",
      "0.0046876962296664715\n",
      "0.004775899928063154\n",
      "0.004653807729482651\n",
      "0.004683793988078833\n",
      "0.004538883455097675\n",
      "0.004552751779556274\n",
      "0.004314345773309469\n",
      "0.004386396612972021\n",
      "0.004719952121376991\n",
      "0.00428122328594327\n",
      "0.00431527104228735\n",
      "0.004270084667950869\n",
      "0.0045753829181194305\n",
      "0.0044023143127560616\n",
      "0.0039484151639044285\n",
      "0.004103608895093203\n",
      "0.003980270121246576\n",
      "0.004027445800602436\n",
      "0.003860018216073513\n",
      "0.004099435638636351\n",
      "0.00402242923155427\n",
      "0.00400903495028615\n",
      "0.003926029894500971\n",
      "0.004093554336577654\n",
      "0.0038519182708114386\n",
      "0.0038985456340014935\n",
      "0.003817212302237749\n",
      "0.0038891909644007683\n",
      "0.003999321721494198\n",
      "0.0036994374822825193\n",
      "0.0037628228310495615\n",
      "0.003779547056183219\n",
      "0.0037352745421230793\n",
      "0.0033358456566929817\n",
      "0.0034196926280856133\n",
      "0.0033775519113987684\n",
      "0.003626168705523014\n",
      "0.0037561526987701654\n",
      "0.003375768195837736\n",
      "0.0033326498232781887\n",
      "0.003390268422663212\n",
      "0.003487239358946681\n",
      "0.003693197388201952\n",
      "0.003338973969221115\n",
      "0.0032719268929213285\n",
      "0.003340723691508174\n",
      "0.0032926988787949085\n",
      "0.0035470020957291126\n",
      "0.0031922522466629744\n",
      "0.0030118536669760942\n",
      "0.003210296155884862\n",
      "0.0032443099189549685\n",
      "0.0032040977384895086\n",
      "0.003025318728759885\n",
      "0.003047945909202099\n",
      "0.0030436876695603132\n",
      "0.002978528616949916\n",
      "0.002899398095905781\n",
      "0.0031419305596500635\n",
      "0.0031651146709918976\n",
      "0.003196794306859374\n",
      "0.0031828777864575386\n",
      "0.003045456251129508\n",
      "0.0027709477581083775\n",
      "0.0027614443097263575\n",
      "0.002902908716350794\n",
      "0.0027292536105960608\n",
      "0.0027185198850929737\n",
      "0.0030084624886512756\n",
      "0.0029146703891456127\n",
      "0.002915182849392295\n",
      "0.0028259875252842903\n",
      "0.002645886968821287\n",
      "0.0028120731003582478\n",
      "0.0026265671476721764\n",
      "0.0026201559230685234\n",
      "0.002672688104212284\n",
      "0.002663154387846589\n",
      "0.0027083551976829767\n",
      "0.0028926916420459747\n",
      "0.002741952193900943\n",
      "0.002862389199435711\n",
      "0.0027345987036824226\n",
      "0.0026617725379765034\n",
      "0.0028808144852519035\n",
      "0.00271618808619678\n",
      "0.0023771042469888926\n",
      "0.0023528775200247765\n",
      "0.0024230200797319412\n",
      "0.0023641183506697416\n",
      "0.002296537859365344\n",
      "0.002468232996761799\n",
      "0.0024287784472107887\n",
      "0.002353665651753545\n",
      "0.002435987116768956\n",
      "0.0024102365132421255\n",
      "0.0023811152204871178\n",
      "0.0024012273643165827\n",
      "0.0024564904160797596\n",
      "0.002683719852939248\n",
      "0.002343375701457262\n",
      "0.002218610839918256\n",
      "0.0024921256117522717\n",
      "0.0026011543814092875\n",
      "0.0025575244799256325\n",
      "0.0024000343400985003\n",
      "0.0022026572842150927\n",
      "0.0020304194185882807\n",
      "0.0021911405492573977\n",
      "0.0023999346885830164\n",
      "0.0025697683449834585\n",
      "0.0020852263551205397\n",
      "0.002451887121424079\n",
      "0.002033000811934471\n",
      "0.0022275608498603106\n",
      "0.002195281209424138\n",
      "0.0023081626277416945\n",
      "0.001997096696868539\n",
      "0.0019263450521975756\n",
      "0.0022884488571435213\n",
      "0.002139696618542075\n",
      "0.0022756997495889664\n",
      "0.0022826262284070253\n",
      "0.0023628943599760532\n",
      "0.0020622704178094864\n",
      "0.0019455383298918605\n",
      "0.0019510762067511678\n",
      "0.001975948689505458\n",
      "0.0019636587239801884\n",
      "0.0018800449324771762\n",
      "0.001863132114522159\n",
      "0.0018757919315248728\n",
      "0.0018379604443907738\n",
      "0.0018494522664695978\n",
      "0.0017712839180603623\n",
      "0.0019176738569512963\n",
      "0.002092568902298808\n",
      "0.0018093225080519915\n",
      "0.0019504239317029715\n",
      "0.0017921699909493327\n",
      "0.0019152762833982706\n",
      "0.0018935962580144405\n",
      "0.0017564452718943357\n",
      "0.0020151555072516203\n",
      "0.002096601063385606\n",
      "0.001721818814985454\n",
      "0.0016839613672345877\n",
      "0.002006198512390256\n",
      "0.001789496629498899\n",
      "0.0017076833173632622\n",
      "0.0018188708927482367\n",
      "0.001623240066692233\n",
      "0.0016117654740810394\n",
      "0.0018048391211777925\n",
      "0.0016266488237306476\n",
      "0.0018793605268001556\n",
      "0.002072019036859274\n",
      "0.00145596987567842\n",
      "0.0015333117917180061\n",
      "0.0016352753154933453\n",
      "0.001638207701034844\n",
      "0.001638738438487053\n",
      "0.0017417159397155046\n",
      "0.0016694271471351385\n",
      "0.0018772402545437217\n",
      "0.0016291332431137562\n",
      "0.0018089513760060072\n",
      "0.0019664994906634092\n",
      "0.0018616252345964313\n",
      "0.001377539592795074\n",
      "0.0015719683142378926\n",
      "0.0015268767019733787\n",
      "0.0014787514228373766\n",
      "0.0016584460390731692\n",
      "0.0014567121397703886\n",
      "0.0014141349820420146\n",
      "0.001721691689454019\n",
      "0.0015863822773098946\n",
      "0.001493006362579763\n",
      "0.0014519228134304285\n",
      "0.0014912120532244444\n",
      "0.0015442395815625787\n",
      "0.0018905411707237363\n",
      "0.001479813363403082\n",
      "0.00141212809830904\n",
      "0.0015478826826438308\n",
      "0.0015891704242676497\n",
      "0.001862503937445581\n",
      "0.001544258208014071\n",
      "0.0016150566516444087\n",
      "0.001621021656319499\n",
      "0.00130336987785995\n",
      "0.0016697855899110436\n",
      "0.0016751672374084592\n",
      "0.0014256108552217484\n",
      "0.001397324725985527\n",
      "0.0015448711346834898\n",
      "0.0014053023187443614\n",
      "0.001299454947002232\n",
      "0.001228051958605647\n",
      "0.0014589325292035937\n",
      "0.0012588197132572532\n",
      "0.0013391923857852817\n",
      "0.0013271658681333065\n",
      "0.001462070271372795\n",
      "0.0013174376217648387\n",
      "0.0012223398080095649\n",
      "0.001289783394895494\n",
      "0.0012730133021250367\n",
      "0.0011833520838990808\n",
      "0.0012264613760635257\n",
      "0.0012688759015873075\n",
      "0.0012183288345113397\n",
      "0.001388740842230618\n",
      "0.0012551845284178853\n",
      "0.001284219790250063\n",
      "0.0011493224883452058\n",
      "0.0012963933404535055\n",
      "0.0013628937304019928\n",
      "0.0012917390558868647\n",
      "0.001311725820414722\n",
      "0.0011351217981427908\n",
      "0.001298626884818077\n",
      "0.0012132070260122418\n",
      "0.00140410999301821\n",
      "0.0012559079332277179\n",
      "0.0012215117458254099\n",
      "0.0012786894803866744\n",
      "0.001419174368493259\n",
      "0.0011512303026393056\n",
      "0.0011222705943509936\n",
      "0.001163835171610117\n",
      "0.0012971254764124751\n",
      "0.001026561832986772\n",
      "0.0013349780347198248\n",
      "0.0012079398147761822\n",
      "0.0012253179447725415\n",
      "0.0012921688612550497\n",
      "0.001060068723745644\n",
      "0.0012982637854292989\n",
      "0.0011324243387207389\n",
      "0.0010984709952026606\n",
      "0.001304969540797174\n",
      "0.0010645602596923709\n",
      "0.0012424219166859984\n",
      "0.0010841882321983576\n",
      "0.0010869803372770548\n",
      "0.0011290594702586532\n",
      "0.0009951777756214142\n",
      "0.0012393519282341003\n",
      "0.001163740991614759\n",
      "0.0009758656960912049\n",
      "0.0012721993261948228\n",
      "0.0010651116026565433\n",
      "0.0009979577735066414\n",
      "0.0009869838831946254\n",
      "0.0012446354376152158\n",
      "0.0010195134673267603\n",
      "0.0011481059482321143\n",
      "0.0012222666991874576\n",
      "0.0009345994330942631\n",
      "0.0011816873447969556\n",
      "0.00108623958658427\n",
      "0.0010284496238455176\n",
      "0.001000867923721671\n",
      "0.0009461866575293243\n",
      "0.001169670489616692\n",
      "0.0011482501868158579\n",
      "0.0010133242467418313\n",
      "0.0010155269410461187\n",
      "0.0009634590242058039\n",
      "0.0011924166465178132\n",
      "0.0011375650065019727\n",
      "0.0009840582497417927\n",
      "0.0011778196785598993\n",
      "0.0011265660868957639\n",
      "0.0011934104841202497\n",
      "0.0010893899016082287\n",
      "0.0010643239365890622\n",
      "0.001059414353221655\n",
      "0.001030842075124383\n",
      "0.0011710523394867778\n",
      "0.0009258553618565202\n",
      "0.001088166143745184\n",
      "0.000836724997498095\n",
      "0.001006297767162323\n",
      "0.00115526735316962\n",
      "0.0008507114253006876\n",
      "0.0010921837529167533\n",
      "0.000987761770375073\n",
      "0.0011446366552263498\n",
      "0.000903285457752645\n",
      "0.001135461963713169\n",
      "0.0008671360556036234\n",
      "0.0009132206323556602\n",
      "0.0009643880184739828\n",
      "0.000982928671874106\n",
      "0.0009415691602043808\n",
      "0.0010285332100465894\n",
      "0.0009195597958751023\n",
      "0.0009877578122541308\n",
      "0.0008794284076429904\n",
      "0.0008520747069269419\n",
      "0.0009218935738317668\n",
      "0.0010503241792321205\n",
      "0.0007721324218437076\n",
      "0.0008814785396680236\n",
      "0.0010561207309365273\n",
      "0.0007961237570270896\n",
      "0.0008576197433285415\n",
      "0.0008696322911418974\n",
      "0.001004075980745256\n",
      "0.0009512025280855596\n",
      "0.00078261096496135\n",
      "0.0009315707720816135\n",
      "0.0009746330324560404\n",
      "0.0008648212533444166\n",
      "0.0009516772697679698\n",
      "0.0009422078146599233\n",
      "0.0007786914356984198\n",
      "0.0008691595867276192\n",
      "0.0008895854698494077\n",
      "0.0009925066260620952\n",
      "0.0007545119151473045\n",
      "0.0008846766431815922\n",
      "0.0008143201703205705\n",
      "0.0007998708751983941\n",
      "0.0008543478907085955\n",
      "0.0007971660816110671\n",
      "0.0008359213825315237\n",
      "0.0009653090965002775\n",
      "0.0007374712149612606\n",
      "0.000918281904887408\n",
      "0.0008317853207699955\n",
      "0.000800293346401304\n",
      "0.0007719394052401185\n",
      "0.0007999909576028585\n",
      "0.0008565373136661947\n",
      "0.0009153453866019845\n",
      "0.0007454419392161071\n",
      "0.0007907695253379643\n",
      "0.000821472960524261\n",
      "0.0008135819807648659\n",
      "0.0008225583587773144\n",
      "0.0007714109378866851\n",
      "0.0010525528341531754\n",
      "0.0007276356336660683\n",
      "0.000979496049694717\n",
      "0.0006350657204166055\n",
      "0.0008052835473790765\n",
      "0.0008990933420136571\n",
      "0.0007284088060259819\n",
      "0.0009578547906130552\n",
      "0.0006487005157396197\n",
      "0.000842458859551698\n",
      "0.0007661469862796366\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(transformer.parameters(), lr=1e-4)\n",
    "n_epochs = 500\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in trange(n_epochs): \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        sequences = batch[1]\n",
    "        tokens, mask, _, _, _ = batch_encode_sequences(sequences)\n",
    "\n",
    "        x = batch[0]\n",
    "        if embedder != \"esmfold\":\n",
    "            x = x[:, 1:-1, :]\n",
    "        mask = mask.bool()\n",
    "        x, mask = x.to(device), mask.to(device)\n",
    "        \n",
    "        # noise = torch.randn_like(x)\n",
    "        # x_noised = x + noise\n",
    "        # output = transformer(x_noised, mask)\n",
    "        output = transformer(x, mask)\n",
    "        loss = F.mse_loss(x, output)\n",
    "        losses.append(loss.item())\n",
    "        if i % 50 == 0: print(loss.item()) \n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed73124-df33-40d0-9091-14d34c66bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56d600-c7a2-4d2a-9209-dbd385cd5a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
