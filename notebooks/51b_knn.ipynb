{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4edd5c-cdf4-4773-b099-3cff64cd62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91149db5-d71c-416b-91cc-4fd1b1ad5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "import einops\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from plaid.compression.hourglass_vq import HourglassVQLightningModule\n",
    "from plaid.esmfold.misc import batch_encode_sequences\n",
    "from plaid.datasets import CATHShardedDataModule\n",
    "from plaid.transforms import trim_or_pad_batch_first\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    no_compress: bool = False\n",
    "    shard_dir: str = \"/data/lux70/data/cath/shards\"\n",
    "    cath_metadata_fpath: str = \"/data/lux70/data/cath/description/cath-domain-list-S35.txt\"\n",
    "    ckpt_dir: str = \"/data/lux70/plaid/checkpoints/hourglass_vq/\"\n",
    "    compression_id: str = \"identity\"\n",
    "    seq_len: int = 512\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d241e2-8f3f-40e9-b9a3-30c8d1ef6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hourglass(ckpt_dir, compression_id):\n",
    "    ckpt_dir = Path(ckpt_dir)\n",
    "    ckpt_fpath = ckpt_dir / compression_id / \"last.ckpt\"\n",
    "    hourglass = HourglassVQLightningModule.load_from_checkpoint(ckpt_fpath)\n",
    "\n",
    "    hourglass.eval()\n",
    "    for param in hourglass.parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    hourglass.to(device)\n",
    "    return hourglass\n",
    "\n",
    "\n",
    "def load_cath_metadata(fpath):\n",
    "    df = pd.read_csv(fpath, sep=\"\\s+\", header=None)\n",
    "    \n",
    "    # from the README file\n",
    "    columns = [\n",
    "        \"cath_id\", # original name: \"CATH domain name (seven characters)\",\n",
    "        \"Class number\",\n",
    "        \"Architecture number\",\n",
    "        \"Topology number\",\n",
    "        \"Homologous superfamily number\",\n",
    "        \"S35 sequence cluster number\",\n",
    "        \"S60 sequence cluster number\",\n",
    "        \"S95 sequence cluster number\",\n",
    "        \"S100 sequence cluster number\",\n",
    "        \"S100 sequence count number\",\n",
    "        \"Domain length\",\n",
    "        \"Structure resolution (Angstroms)\"\n",
    "    ]\n",
    "    \n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_cath_cache_dataloaders(shard_dir, seq_len):\n",
    "    dm = CATHShardedDataModule(\n",
    "        shard_dir=shard_dir,\n",
    "        seq_len=seq_len,\n",
    "    )\n",
    "    dm.setup()\n",
    "\n",
    "    train_dataloader = dm.train_dataloader()\n",
    "    val_dataloader = dm.val_dataloader()\n",
    "    print(len(train_dataloader.dataset))\n",
    "    print(len(val_dataloader.dataset))\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "def collect_batches(dataloader, hourglass, compress=True, max_length=512):\n",
    "    sequences = []\n",
    "    cath_ids = []\n",
    "    \n",
    "    all_x_c = []\n",
    "    all_m_d = []\n",
    "\n",
    "    print(\"dataset length:\", len(dataloader.dataset))\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        x, sequence, header = batch\n",
    "        sequences.extend(sequence)\n",
    "        cath_ids.extend(header)\n",
    "        \n",
    "        aatype, mask, _, _, _ = batch_encode_sequences(sequence)\n",
    "    \n",
    "        if not compress:\n",
    "            assert hourglass is None\n",
    "            x_c = x\n",
    "            m_d = mask\n",
    "        else:\n",
    "            x = x.to(device)\n",
    "            mask = mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                x_c, m_d = hourglass(x, mask.bool(), infer_only=True)\n",
    "\n",
    "        x_c = trim_or_pad_batch_first(x_c, max_length)\n",
    "        m_d = trim_or_pad_batch_first(m_d, max_length)\n",
    "\n",
    "        all_x_c.append(x_c.cpu().numpy())\n",
    "        all_m_d.append(m_d.cpu().numpy())\n",
    "    \n",
    "    all_x_c = np.concatenate(all_x_c, axis=0)\n",
    "    all_m_d = np.concatenate(all_m_d, axis=0)\n",
    "\n",
    "    md_broadcast = einops.repeat(all_m_d, \"n l -> n l c\", c = all_x_c.shape[-1])\n",
    "    xc_pooled = (all_x_c * md_broadcast).sum(axis=1) / md_broadcast.sum(axis=1)\n",
    "    \n",
    "    return {\n",
    "        \"x\": all_x_c,\n",
    "        \"mask\": all_m_d,\n",
    "        \"sequences\": sequences,\n",
    "        \"cath_ids\": cath_ids,\n",
    "        \"x_pooled\": xc_pooled\n",
    "    }\n",
    "\n",
    "\n",
    "def run_knn(n_neighbors, target, train_data, train_df, val_data, val_df):\n",
    "    knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X=train_data['x_pooled_ordered'], y=train_df[target].values)\n",
    "    pred_classes = knn.predict(val_data['x_pooled_ordered'])\n",
    "    correct = (pred_classes == val_df[target].values).sum() / len(val_df)\n",
    "    return correct \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8caf2b5c-df47-4a5e-bb67-c1ec991d3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, compression_id):\n",
    "    print(\"Compression id\", compression_id)\n",
    "    args.compression_id = compression_id\n",
    "    \n",
    "    df = load_cath_metadata(args.cath_metadata_fpath)\n",
    "    if not args.no_compress:\n",
    "        hourglass = load_hourglass(args.ckpt_dir, args.compression_id)\n",
    "        shorten_factor = hourglass.enc.shorten_factor\n",
    "        downproj_factor = hourglass.enc.downproj_factor\n",
    "    else:\n",
    "        hourglass = None\n",
    "        shorten_factor = 1\n",
    "        downproj_factor = 1\n",
    "    \n",
    "    train_dataloader, val_dataloader = load_cath_cache_dataloaders(args.shard_dir, args.seq_len)\n",
    "    \n",
    "    compress = not args.no_compress\n",
    "    val_data = collect_batches(val_dataloader, hourglass, compress, args.seq_len)\n",
    "    train_data = collect_batches(train_dataloader, hourglass, compress, args.seq_len)\n",
    "    \n",
    "    # create an dataframe to make it easier to manipulate\n",
    "    train_embed_df = pd.DataFrame({\"embedding_idx\": np.arange(len(train_data['cath_ids'])), \"cath_id\": train_data['cath_ids']})\n",
    "    val_embed_df = pd.DataFrame({\"embedding_idx\": np.arange(len(val_data['cath_ids'])), \"cath_id\": val_data['cath_ids']})\n",
    "    \n",
    "    # filter such that we only keep those with both metadata and cath_ids\n",
    "    train_df = df[df.cath_id.isin(train_data['cath_ids'])]\n",
    "    val_df = df[df.cath_id.isin(val_data['cath_ids'])]\n",
    "    \n",
    "    # join the dataframes\n",
    "    train_df = train_df.set_index(\"cath_id\").join(train_embed_df.set_index(\"cath_id\"), how='left', rsuffix=\"embed_\")\n",
    "    val_df = val_df.set_index(\"cath_id\").join(val_embed_df.set_index(\"cath_id\"), how='left', rsuffix=\"embed_\")\n",
    "    \n",
    "    train_df = train_df[~train_df.embedding_idx.isna()]\n",
    "    val_df = val_df[~val_df.embedding_idx.isna()]\n",
    "    \n",
    "    # reorder the pooled embedding\n",
    "    train_data['x_pooled_ordered'] = train_data['x_pooled'][train_df.embedding_idx.values]\n",
    "    val_data['x_pooled_ordered'] = val_data['x_pooled'][val_df.embedding_idx.values]\n",
    "    \n",
    "    # Run knn experiments:\n",
    "    results = pd.DataFrame(\n",
    "        {\"compression_model_id\": [],\n",
    "        \"shorten_factor:\": [],\n",
    "        \"downprojection_factor\": [],\n",
    "        \"n_neighbors\": [],\n",
    "        \"pred_target\": [],\n",
    "        \"acc\": []}\n",
    "    )\n",
    "    \n",
    "    for n_neighbors in [1, 5]:\n",
    "        for target in [\"Class number\", \"Architecture number\", \"Topology number\", \"Homologous superfamily number\"]:\n",
    "            print(\"n_neighbors:\", n_neighbors, \"target:\", target)\n",
    "            correct = run_knn(n_neighbors, target, train_data, train_df, val_data, val_df)\n",
    "    \n",
    "            row = pd.DataFrame(\n",
    "                {\n",
    "                    \"compression_model_id\": [args.compression_id],\n",
    "                    \"shorten_factor:\": [shorten_factor],\n",
    "                    \"downprojection_factor\": [downproj_factor],\n",
    "                    \"n_neighbors\": [n_neighbors],\n",
    "                    \"pred_target\": [target],\n",
    "                    \"acc\": [correct]\n",
    "                }\n",
    "            )\n",
    "            results = pd.concat([results, row])\n",
    "    \n",
    "    results.to_csv(f\"/data/lux70/plaid/artifacts/eval/cath_knn/{args.compression_id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5955a-1eb7-4f35-b814-e3b655bad84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression id g8e83omk\n",
      "using quantizer tanh\n",
      "25508\n",
      "6377\n",
      "dataset length: 6377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:35<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 25508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 798/798 [02:24<00:00,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "for compression_id in ['g8e83omk', '7str7fhl', 'ich20c3q', 'uhg29zk4', '13lltqha', 'fbbrfqzk', 'kyytc8i9', 'mm9fe6x9', '8ebs7j9h']:\n",
    "    main(args, compression_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0319fd-f4bc-4853-a1eb-39a372e32206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plaid)",
   "language": "python",
   "name": "plaid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
