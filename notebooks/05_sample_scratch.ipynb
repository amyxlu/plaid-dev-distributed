{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import accelerate\n",
    "\n",
    "# import safetensors.torch as safetorch\n",
    "import torch\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import k_diffusion as K\n",
    "from k_diffusion.models.esmfold import ESMFOLD_S_DIM\n",
    "from k_diffusion.proteins import LatentToSequence, LatentToStructure\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = np.load(\"/home/amyxlu/kdiffusion/artifacts/bcjgpe29/step7000_sampled.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 512, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.201468 188.12323 -2162.4023 13751.117\n"
     ]
    }
   ],
   "source": [
    "print(sampled.mean(), sampled.std(), sampled.min(), sampled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = K.utils.to_tensor(sampled[:, :128, :], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_constructor = LatentToSequence(device=device)\n",
    "sequence_probs, sequence_idx, sequence_str = sequence_constructor.to_sequence(\n",
    "    x_0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 3142.748046875 MB\n",
      "Current CUDA memory reserved: 3158.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):   3%|▎         | 1/32 [03:43<1:55:23, 223.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4366.29931640625 MB\n",
      "Current CUDA memory reserved: 4590.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):   6%|▋         | 2/32 [07:26<1:51:34, 223.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4367.29931640625 MB\n",
      "Current CUDA memory reserved: 4850.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):   9%|▉         | 3/32 [11:06<1:47:09, 221.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4368.04931640625 MB\n",
      "Current CUDA memory reserved: 5106.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):  12%|█▎        | 4/32 [14:44<1:42:44, 220.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4367.04931640625 MB\n",
      "Current CUDA memory reserved: 5086.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):  16%|█▌        | 5/32 [18:27<1:39:34, 221.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4368.04931640625 MB\n",
      "Current CUDA memory reserved: 4702.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):  19%|█▉        | 6/32 [22:06<1:35:35, 220.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4366.79931640625 MB\n",
      "Current CUDA memory reserved: 4958.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):  22%|██▏       | 7/32 [25:47<1:31:58, 220.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4366.54931640625 MB\n",
      "Current CUDA memory reserved: 4956.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):  25%|██▌       | 8/32 [29:31<1:28:42, 221.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4367.17431640625 MB\n",
      "Current CUDA memory reserved: 5214.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):  28%|██▊       | 9/32 [33:13<1:24:58, 221.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4367.04931640625 MB\n",
      "Current CUDA memory reserved: 4702.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):  31%|███▏      | 10/32 [36:51<1:20:52, 220.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4366.29931640625 MB\n",
      "Current CUDA memory reserved: 4958.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):  34%|███▍      | 11/32 [40:31<1:17:09, 220.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4365.45556640625 MB\n",
      "Current CUDA memory reserved: 4958.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):  38%|███▊      | 12/32 [44:11<1:13:24, 220.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA memory allocated: 4366.29931640625 MB\n",
      "Current CUDA memory reserved: 4958.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Generating structure from latents..):  38%|███▊      | 12/32 [47:01<1:18:22, 235.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amyxlu/kdiffusion/experiments/_sample_scratch.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx5/home/amyxlu/kdiffusion/experiments/_sample_scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m structure_constructor \u001b[39m=\u001b[39m LatentToStructure(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdgx5/home/amyxlu/kdiffusion/experiments/_sample_scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m pdb_strs, metrics \u001b[39m=\u001b[39m structure_constructor\u001b[39m.\u001b[39;49mto_structure(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx5/home/amyxlu/kdiffusion/experiments/_sample_scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     x_0,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx5/home/amyxlu/kdiffusion/experiments/_sample_scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     sequences\u001b[39m=\u001b[39;49msequence_str,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx5/home/amyxlu/kdiffusion/experiments/_sample_scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     num_recycles\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx5/home/amyxlu/kdiffusion/experiments/_sample_scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx5/home/amyxlu/kdiffusion/experiments/_sample_scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/kdiffusion/k_diffusion/proteins.py:283\u001b[0m, in \u001b[0;36mto_structure\u001b[0;34m(self, latent, sequences, num_recycles, batch_size)\u001b[0m\n\u001b[1;32m    275\u001b[0m z_ \u001b[39m=\u001b[39m latent\u001b[39m.\u001b[39mnew_zeros(s_\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], L, L, ESMFOLD_Z_DIM)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    277\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    278\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mesmfold\u001b[39m.\u001b[39mfolding_trunk(\n\u001b[1;32m    279\u001b[0m         s_s_0\u001b[39m=\u001b[39ms_,\n\u001b[1;32m    280\u001b[0m         s_z_0\u001b[39m=\u001b[39mz_,\n\u001b[1;32m    281\u001b[0m         aa\u001b[39m=\u001b[39maa_,\n\u001b[1;32m    282\u001b[0m         residx\u001b[39m=\u001b[39mresidx_,\n\u001b[0;32m--> 283\u001b[0m         mask\u001b[39m=\u001b[39mmask_,\n\u001b[1;32m    284\u001b[0m         num_recycles\u001b[39m=\u001b[39mnum_recycles,\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m metrics\u001b[39m.\u001b[39mappend(outputs_to_avg_metric(output))\n\u001b[1;32m    287\u001b[0m all_pdb_strs\u001b[39m.\u001b[39mextend(output_to_pdb(output))\n",
      "File \u001b[0;32m~/kdiffusion/k_diffusion/models/esmfold/esmfold.py:246\u001b[0m, in \u001b[0;36mfolding_trunk\u001b[0;34m(self, s_s_0, s_z_0, aa, residx, mask, num_recycles)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfolding_trunk\u001b[39m(\u001b[39mself\u001b[39m, s_s_0, s_z_0, aa, residx, mask, num_recycles: T\u001b[39m.\u001b[39mOptional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    245\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrunk \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     B, L \u001b[39m=\u001b[39m aa\u001b[39m.\u001b[39mshape\n\u001b[1;32m    247\u001b[0m     structure: \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrunk(\n\u001b[1;32m    248\u001b[0m         s_s_0, s_z_0, aa, residx, mask, no_recycles\u001b[39m=\u001b[39mnum_recycles\n\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    250\u001b[0m     \u001b[39m# Documenting what we expect:\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/kdiffusion/k_diffusion/models/esmfold/trunk.py:202\u001b[0m, in \u001b[0;36mFoldingTrunk.forward\u001b[0;34m(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles)\u001b[0m\n\u001b[1;32m    199\u001b[0m recycle_z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecycle_z_norm(recycle_z\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m    200\u001b[0m recycle_z \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecycle_disto(recycle_bins\u001b[39m.\u001b[39mdetach())\n\u001b[0;32m--> 202\u001b[0m s_s, s_z \u001b[39m=\u001b[39m trunk_iter(s_s_0 \u001b[39m+\u001b[39;49m recycle_s, s_z_0 \u001b[39m+\u001b[39;49m recycle_z, residx, mask)\n\u001b[1;32m    204\u001b[0m \u001b[39m# === Structure module ===\u001b[39;00m\n\u001b[1;32m    205\u001b[0m structure \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstructure_module(\n\u001b[1;32m    206\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39msingle\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrunk2sm_s(s_s), \u001b[39m\"\u001b[39m\u001b[39mpair\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrunk2sm_z(s_z)},\n\u001b[1;32m    207\u001b[0m     true_aa,\n\u001b[1;32m    208\u001b[0m     mask\u001b[39m.\u001b[39mto(s_s\u001b[39m.\u001b[39mdtype),\n\u001b[1;32m    209\u001b[0m )\n",
      "File \u001b[0;32m~/kdiffusion/k_diffusion/models/esmfold/trunk.py:185\u001b[0m, in \u001b[0;36mFoldingTrunk.forward.<locals>.trunk_iter\u001b[0;34m(s, z, residx, mask)\u001b[0m\n\u001b[1;32m    182\u001b[0m z \u001b[39m=\u001b[39m z \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpairwise_positional_embedding(residx, mask\u001b[39m=\u001b[39mmask)\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 185\u001b[0m     s, z \u001b[39m=\u001b[39m block(s, z, mask\u001b[39m=\u001b[39;49mmask, residue_index\u001b[39m=\u001b[39;49mresidx, chunk_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size)\n\u001b[1;32m    186\u001b[0m \u001b[39mreturn\u001b[39;00m s, z\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/kdiffusion/k_diffusion/models/esmfold/tri_self_attn_block.py:154\u001b[0m, in \u001b[0;36mTriangularSelfAttentionBlock.forward\u001b[0;34m(self, sequence_state, pairwise_state, mask, chunk_size, **_TriangularSelfAttentionBlock__kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m pairwise_state \u001b[39m=\u001b[39m pairwise_state \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_drop(\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtri_mul_in(pairwise_state, mask\u001b[39m=\u001b[39mtri_mask)\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    150\u001b[0m pairwise_state \u001b[39m=\u001b[39m pairwise_state \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow_drop(\n\u001b[1;32m    151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtri_att_start(pairwise_state, mask\u001b[39m=\u001b[39mtri_mask, chunk_size\u001b[39m=\u001b[39mchunk_size)\n\u001b[1;32m    152\u001b[0m )\n\u001b[1;32m    153\u001b[0m pairwise_state \u001b[39m=\u001b[39m pairwise_state \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_drop(\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtri_att_end(pairwise_state, mask\u001b[39m=\u001b[39;49mtri_mask, chunk_size\u001b[39m=\u001b[39;49mchunk_size)\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[39m# MLP over pairs.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m pairwise_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_pair(pairwise_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/openfold/openfold/model/triangular_attention.py:128\u001b[0m, in \u001b[0;36mTriangleAttention.forward\u001b[0;34m(self, x, mask, chunk_size, use_memory_efficient_kernel, use_lma, inplace_safe)\u001b[0m\n\u001b[1;32m    125\u001b[0m biases \u001b[39m=\u001b[39m [mask_bias, triangle_bias]\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m chunk_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_chunk(\n\u001b[1;32m    129\u001b[0m         x, \n\u001b[1;32m    130\u001b[0m         biases, \n\u001b[1;32m    131\u001b[0m         chunk_size, \n\u001b[1;32m    132\u001b[0m         use_memory_efficient_kernel\u001b[39m=\u001b[39;49muse_memory_efficient_kernel,\n\u001b[1;32m    133\u001b[0m         use_lma\u001b[39m=\u001b[39;49muse_lma,\n\u001b[1;32m    134\u001b[0m         inplace_safe\u001b[39m=\u001b[39;49minplace_safe,\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmha(\n\u001b[1;32m    138\u001b[0m         q_x\u001b[39m=\u001b[39mx, \n\u001b[1;32m    139\u001b[0m         kv_x\u001b[39m=\u001b[39mx, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m         use_lma\u001b[39m=\u001b[39muse_lma\n\u001b[1;32m    143\u001b[0m     )\n",
      "File \u001b[0;32m~/openfold/openfold/model/triangular_attention.py:76\u001b[0m, in \u001b[0;36mTriangleAttention._chunk\u001b[0;34m(self, x, biases, chunk_size, use_memory_efficient_kernel, use_lma, inplace_safe)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mtriangle! triangle!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m mha_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m     71\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mq_x\u001b[39m\u001b[39m\"\u001b[39m: x,\n\u001b[1;32m     72\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkv_x\u001b[39m\u001b[39m\"\u001b[39m: x,\n\u001b[1;32m     73\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbiases\u001b[39m\u001b[39m\"\u001b[39m: biases,\n\u001b[1;32m     74\u001b[0m }\n\u001b[0;32m---> 76\u001b[0m \u001b[39mreturn\u001b[39;00m chunk_layer(\n\u001b[1;32m     77\u001b[0m     partial(\n\u001b[1;32m     78\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha, \n\u001b[1;32m     79\u001b[0m         use_memory_efficient_kernel\u001b[39m=\u001b[39;49muse_memory_efficient_kernel,\n\u001b[1;32m     80\u001b[0m         use_lma\u001b[39m=\u001b[39;49muse_lma\n\u001b[1;32m     81\u001b[0m     ),\n\u001b[1;32m     82\u001b[0m     mha_inputs,\n\u001b[1;32m     83\u001b[0m     chunk_size\u001b[39m=\u001b[39;49mchunk_size,\n\u001b[1;32m     84\u001b[0m     no_batch_dims\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(x\u001b[39m.\u001b[39;49mshape[:\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m]),\n\u001b[1;32m     85\u001b[0m     _out\u001b[39m=\u001b[39;49mx \u001b[39mif\u001b[39;49;00m inplace_safe \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     86\u001b[0m )\n",
      "File \u001b[0;32m~/openfold/openfold/utils/chunk_utils.py:299\u001b[0m, in \u001b[0;36mchunk_layer\u001b[0;34m(layer, inputs, chunk_size, no_batch_dims, low_mem, _out, _add_into_out)\u001b[0m\n\u001b[1;32m    296\u001b[0m chunks \u001b[39m=\u001b[39m tensor_tree_map(select_chunk, prepped_inputs)\n\u001b[1;32m    298\u001b[0m \u001b[39m# Run the layer on the chunk\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m output_chunk \u001b[39m=\u001b[39m layer(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mchunks)\n\u001b[1;32m    301\u001b[0m \u001b[39m# Allocate space for the output\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/openfold/openfold/model/primitives.py:507\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, q_x, kv_x, biases, use_memory_efficient_kernel, use_lma, lma_q_chunk_size, lma_kv_chunk_size, use_flash, flash_mask)\u001b[0m\n\u001b[1;32m    504\u001b[0m     o \u001b[39m=\u001b[39m _attention(q, k, v, biases)\n\u001b[1;32m    505\u001b[0m     o \u001b[39m=\u001b[39m o\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m--> 507\u001b[0m o \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_up(o, q_x)\n\u001b[1;32m    509\u001b[0m \u001b[39mreturn\u001b[39;00m o\n",
      "File \u001b[0;32m~/openfold/openfold/model/primitives.py:408\u001b[0m, in \u001b[0;36mAttention._wrap_up\u001b[0;34m(self, o, q_x)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_up\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    404\u001b[0m     o: torch\u001b[39m.\u001b[39mTensor, \n\u001b[1;32m    405\u001b[0m     q_x: torch\u001b[39m.\u001b[39mTensor\n\u001b[1;32m    406\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    407\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_g \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 408\u001b[0m         g \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msigmoid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_g(q_x))\n\u001b[1;32m    410\u001b[0m         \u001b[39m# [*, Q, H, C_hidden]\u001b[39;00m\n\u001b[1;32m    411\u001b[0m         g \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mview(g\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_heads, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kdif/lib/python3.11/site-packages/torch/nn/modules/activation.py:292\u001b[0m, in \u001b[0;36mSigmoid.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49msigmoid(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "structure_constructor = LatentToStructure(device=device)\n",
    "pdb_strs, metrics = structure_constructor.to_structure(\n",
    "    x_0,\n",
    "    sequences=sequence_str,\n",
    "    num_recycles=4,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # TODO: log? save? calculate metrics?\n",
    "    # x_0 = K.evaluation.compute_features(\n",
    "    #     accelerator, sample_fn, lambda x: x, args.n, args.batch_size\n",
    "    # )\n",
    "    # return x_0\n",
    "    # if accelerator.is_main_process:\n",
    "    #     filename = f\"{args.prefix}.pkl\"\n",
    "\n",
    "    #     for i, out in enumerate(x_0):\n",
    "    # filename = f'{args.prefix}_{i:05}.png'\n",
    "    # K.utils.to_pil_image(out).save(filename)\n",
    "    return x_0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
