{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069dd14d-c22d-4561-bd01-11c7e950dcb2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5650be11-405d-4851-b4d3-1513433239a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891c3eea-bcfb-4dfd-8cf5-0bb3a1ea5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from plaid.datasets import CATHShardedDataModule\n",
    "from plaid.esmfold.misc import batch_encode_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca9cb3-2120-4211-8abd-7300d04dada1",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "865afd3b-75f7-4687-bdb1-37a108cef252",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_dir = \"/data/lux70/data/rocklin/shards/\"\n",
    "# embedder = \"esmfold\"\n",
    "embedder = \"esm2_t6_8M_UR50D\"\n",
    "D = 1024 if embedder == \"esmfold\" else 320\n",
    "\n",
    "dm = CATHShardedDataModule(\n",
    "    storage_type=\"hdf5\",\n",
    "    shard_dir=shard_dir,\n",
    "    embedder=embedder,\n",
    "    seq_len=256,\n",
    "    batch_size=512\n",
    ")\n",
    "dm.setup()\n",
    "train_dataloader = dm.train_dataloader()\n",
    "val_dataloader = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee1892-7dba-4758-867e-7f8e058933ab",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "22c1557a-7e57-4d9f-94da-153815624c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def pad_to_multiple(tensor, multiple, dim = -1, value = 0):\n",
    "    seq_len = tensor.shape[dim]\n",
    "    m = seq_len / multiple\n",
    "    if m.is_integer():\n",
    "        return tensor\n",
    "    remainder = math.ceil(m) * multiple - seq_len\n",
    "    pad_offset = (0,) * (-1 - dim) * 2\n",
    "    return F.pad(tensor, (*pad_offset, 0, remainder), value = value)\n",
    "\n",
    "def cast_tuple(val, depth = 1):\n",
    "    return val if isinstance(val, tuple) else ((val,) * depth)\n",
    "\n",
    "# factory\n",
    "\n",
    "def get_hourglass_transformer(\n",
    "    dim,\n",
    "    *,\n",
    "    depth,\n",
    "    shorten_factor,\n",
    "    attn_resampling,\n",
    "    updown_sample_type,\n",
    "    **kwargs\n",
    "):\n",
    "    assert isinstance(depth, int) or (isinstance(depth, tuple)  and len(depth) == 3), 'depth must be either an integer or a tuple of 3, indicating (pre_transformer_depth, <nested-hour-glass-config>, post_transformer_depth)'\n",
    "    assert not (isinstance(depth, int) and shorten_factor), 'there does not need to be a shortening factor when only a single transformer block is indicated (depth of one integer value)'\n",
    "\n",
    "    if isinstance(depth, int):\n",
    "        return Transformer(dim = dim, depth = depth, **kwargs)\n",
    "\n",
    "    return HourglassTransformer(dim = dim, depth = depth, shorten_factor = shorten_factor, attn_resampling = attn_resampling, updown_sample_type = updown_sample_type, **kwargs)\n",
    "\n",
    "# up and down sample classes\n",
    "\n",
    "class NaiveDownsample(nn.Module):\n",
    "    def __init__(self, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return reduce(x, 'b (n s) d -> b n d', 'mean', s = self.shorten_factor)\n",
    "\n",
    "class NaiveUpsample(nn.Module):\n",
    "    def __init__(self, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return repeat(x, 'b n d -> b (n s) d', s = self.shorten_factor)\n",
    "\n",
    "class LinearDownsample(nn.Module):\n",
    "    def __init__(self, dim, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim * shorten_factor, dim)\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b (n s) d -> b n (s d)', s = self.shorten_factor)\n",
    "        return self.proj(x)\n",
    "\n",
    "class LinearUpsample(nn.Module):\n",
    "    def __init__(self, dim, shorten_factor):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, dim * shorten_factor)\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return rearrange(x, 'b n (s d) -> b (n s) d', s = self.shorten_factor)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs) + x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        causal = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = heads * dim_head\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h, device = self.heads, x.device\n",
    "        kv_input = default(context, x)\n",
    "\n",
    "        q, k, v = self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "        mask_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b j -> b () () j')\n",
    "            sim = sim.masked_fill(~mask, mask_value)\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = sim.shape[-2:]\n",
    "            mask = torch.ones(i, j, device = device, dtype = torch.bool).triu_(j - i + 1)\n",
    "            mask = rearrange(mask, 'i j -> () () i j')\n",
    "            sim = sim.masked_fill(mask, mask_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "def FeedForward(dim, mult = 4, dropout = 0.):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, dim * mult),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(dim * mult, dim)\n",
    "    )\n",
    "\n",
    "# transformer classes\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth,\n",
    "        causal = False,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_mult = 4,\n",
    "        ff_dropout = 0.,\n",
    "        norm_out = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNormResidual(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout, causal = causal)),\n",
    "                PreNormResidual(dim, FeedForward(dim, mult = ff_mult, dropout = ff_dropout))\n",
    "            ]))\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, context = context, mask = mask)\n",
    "            x = ff(x)\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class HourglassTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth,\n",
    "        shorten_factor = 2,\n",
    "        attn_resampling = True,\n",
    "        updown_sample_type = 'naive',\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        causal = False,\n",
    "        norm_out = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(depth) == 3, 'depth should be a tuple of length 3'\n",
    "        assert updown_sample_type in {'naive', 'linear'}, 'downsample / upsample type must be either naive (average pool and repeat) or linear (linear projection and reshape)'\n",
    "\n",
    "        pre_layers_depth, valley_depth, post_layers_depth = depth\n",
    "\n",
    "        if isinstance(shorten_factor, (tuple, list)):\n",
    "            shorten_factor, *rest_shorten_factor = shorten_factor\n",
    "        elif isinstance(valley_depth, int):\n",
    "            shorten_factor, rest_shorten_factor = shorten_factor, None\n",
    "        else:\n",
    "            shorten_factor, rest_shorten_factor = shorten_factor, shorten_factor\n",
    "\n",
    "        transformer_kwargs = dict(\n",
    "            dim = dim,\n",
    "            heads = heads,\n",
    "            dim_head = dim_head\n",
    "        )\n",
    "\n",
    "        self.causal = causal\n",
    "        self.shorten_factor = shorten_factor\n",
    "\n",
    "        if updown_sample_type == 'naive':\n",
    "            self.downsample = NaiveDownsample(shorten_factor)\n",
    "            self.upsample   = NaiveUpsample(shorten_factor)\n",
    "        elif updown_sample_type == 'linear':\n",
    "            self.downsample = LinearDownsample(dim, shorten_factor)\n",
    "            self.upsample   = LinearUpsample(dim, shorten_factor)\n",
    "        else:\n",
    "            raise ValueError(f'unknown updown_sample_type keyword value - must be either naive or linear for now')\n",
    "\n",
    "        self.valley_transformer = get_hourglass_transformer(\n",
    "            shorten_factor = rest_shorten_factor,\n",
    "            depth = valley_depth,\n",
    "            attn_resampling = attn_resampling,\n",
    "            updown_sample_type = updown_sample_type,\n",
    "            causal = causal,\n",
    "            **transformer_kwargs\n",
    "        )\n",
    "\n",
    "        self.attn_resampling_pre_valley = Transformer(depth = 1, **transformer_kwargs) if attn_resampling else None\n",
    "        self.attn_resampling_post_valley = Transformer(depth = 1, **transformer_kwargs) if attn_resampling else None\n",
    "\n",
    "        self.pre_transformer = Transformer(depth = pre_layers_depth, causal = causal, **transformer_kwargs)\n",
    "        self.post_transformer = Transformer(depth = post_layers_depth, causal = causal, **transformer_kwargs)\n",
    "        self.norm_out = nn.LayerNorm(dim) if norm_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # b : batch, n : sequence length, d : feature dimension, s : shortening factor\n",
    "\n",
    "        s, b, n = self.shorten_factor, *x.shape[:2]\n",
    "\n",
    "        # top half of hourglass, pre-transformer layers\n",
    "\n",
    "        x = self.pre_transformer(x, mask = mask)\n",
    "\n",
    "        # pad to multiple of shortening factor, in preparation for pooling\n",
    "\n",
    "        x = pad_to_multiple(x, s, dim = -2)\n",
    "\n",
    "        if exists(mask):\n",
    "            padded_mask = pad_to_multiple(mask, s, dim = -1, value = False)\n",
    "\n",
    "        # save the residual, and for \"attention resampling\" at downsample and upsample\n",
    "\n",
    "        x_residual = x.clone()\n",
    "\n",
    "        # if autoregressive, do the shift by shortening factor minus one\n",
    "\n",
    "        if self.causal:\n",
    "            shift = s - 1\n",
    "            x = F.pad(x, (0, 0, shift, -shift), value = 0.)\n",
    "\n",
    "            if exists(mask):\n",
    "                padded_mask = F.pad(padded_mask, (shift, -shift), value = False)\n",
    "\n",
    "        # naive average pool\n",
    "\n",
    "        downsampled = self.downsample(x)\n",
    "\n",
    "        if exists(mask):\n",
    "            downsampled_mask = reduce(padded_mask, 'b (n s) -> b n', 'sum', s = s) > 0\n",
    "        else:\n",
    "            downsampled_mask = None\n",
    "\n",
    "        # pre-valley \"attention resampling\" - they have the pooled token in each bucket attend to the tokens pre-pooled\n",
    "\n",
    "        if exists(self.attn_resampling_pre_valley):\n",
    "            if exists(mask):\n",
    "                attn_resampling_mask = rearrange(padded_mask, 'b (n s) -> (b n) s', s = s)\n",
    "            else:\n",
    "                attn_resampling_mask = None\n",
    "\n",
    "            downsampled = self.attn_resampling_pre_valley(\n",
    "                rearrange(downsampled, 'b n d -> (b n) () d'),\n",
    "                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n",
    "                mask = attn_resampling_mask\n",
    "            )\n",
    "\n",
    "            downsampled = rearrange(downsampled, '(b n) () d -> b n d', b = b)\n",
    "\n",
    "        # the \"valley\" - either a regular transformer or another hourglass\n",
    "\n",
    "        x = self.valley_transformer(downsampled, mask = downsampled_mask)\n",
    "\n",
    "        valley_out = x.clone()\n",
    "\n",
    "        # naive repeat upsample\n",
    "\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        # add the residual\n",
    "\n",
    "        x = x + x_residual\n",
    "\n",
    "        # post-valley \"attention resampling\"\n",
    "\n",
    "        if exists(self.attn_resampling_post_valley):\n",
    "            x = self.attn_resampling_post_valley(\n",
    "                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n",
    "                rearrange(valley_out, 'b n d -> (b n) () d')\n",
    "            )\n",
    "\n",
    "            x = rearrange(x, '(b n) s d -> b (n s) d', b = b)\n",
    "\n",
    "        # bring sequence back to original length, if it were padded for pooling\n",
    "\n",
    "        x = x[:, :n]\n",
    "\n",
    "        # post-valley transformers\n",
    "\n",
    "        x = self.post_transformer(x, mask = mask)\n",
    "        return self.norm_out(x)\n",
    "\n",
    "transformer = get_hourglass_transformer(\n",
    "    dim = D,                     # feature dimension\n",
    "    heads = 8,                      # attention heads\n",
    "    dim_head = 64,                  # dimension per attention head\n",
    "    shorten_factor = 2,             # shortening factor\n",
    "    depth = (4, 2, 4),              # tuple of 3, standing for pre-transformer-layers, valley-transformer-layers (after downsample), post-transformer-layers (after upsample) - the valley transformer layers can be yet another nested tuple, in which case it will shorten again recursively\n",
    "    attn_resampling = True,\n",
    "    updown_sample_type = \"naive\",\n",
    "    causal = True,\n",
    "    norm_out = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed71e7-c59e-4b32-8016-97a94ef91d92",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35eaff9-b00d-4e9d-9284-cbe7a2f8168e",
   "metadata": {},
   "source": [
    "## Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1655ed6-be27-430e-8827-e141e3ddef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 43, 320])\n",
      "torch.Size([512, 43])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "sequences = batch[1]\n",
    "tokens, mask, _, _, _ = batch_encode_sequences(sequences)\n",
    "\n",
    "x = batch[0]\n",
    "if embedder != \"esmfold\":\n",
    "    x = x[:, 1:-1, :]\n",
    "mask = mask.bool()\n",
    "print(x.shape)\n",
    "print(mask.shape)\n",
    "\n",
    "device = \"cuda\"\n",
    "transformer = transformer.to(device)\n",
    "x, mask = x.to(device), mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a32f8ce-3ea1-4a15-bb15-5b90e127ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "213b5dac-3e7f-48e6-9d30-269c2cb49bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 43, 320])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3317f-7910-4f08-9ade-87d1aa08be4f",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "22956a45-e3ee-449f-bef0-30467a2d610e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2a83ab3ed54276aa3323d5536097e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035845961421728134\n",
      "0.033326685428619385\n",
      "0.031069863587617874\n",
      "0.029672497883439064\n",
      "0.028546307235956192\n",
      "0.02752477489411831\n",
      "0.026572855189442635\n",
      "0.025748198851943016\n",
      "0.02485238015651703\n",
      "0.024377355352044106\n",
      "0.023246990516781807\n",
      "0.022477811202406883\n",
      "0.021738136187195778\n",
      "0.02120210975408554\n",
      "0.02036917395889759\n",
      "0.019934097304940224\n",
      "0.0191204734146595\n",
      "0.01850868947803974\n",
      "0.01789453998208046\n",
      "0.017309917137026787\n",
      "0.01691448874771595\n",
      "0.016384942457079887\n",
      "0.015899430960416794\n",
      "0.01532022561877966\n",
      "0.014860481023788452\n",
      "0.014414000324904919\n",
      "0.014095748774707317\n",
      "0.013452598825097084\n",
      "0.013421254232525826\n",
      "0.012859363108873367\n",
      "0.01231471449136734\n",
      "0.011976531706750393\n",
      "0.011537332087755203\n",
      "0.011355330236256123\n",
      "0.010985419154167175\n",
      "0.010843458585441113\n",
      "0.010638715699315071\n",
      "0.010002760216593742\n",
      "0.009749649092555046\n",
      "0.009461858309805393\n",
      "0.009264327585697174\n",
      "0.009261995553970337\n",
      "0.008769209496676922\n",
      "0.00890357792377472\n",
      "0.008274618536233902\n",
      "0.00820575188845396\n",
      "0.00798237044364214\n",
      "0.007679465226829052\n",
      "0.0075495135970413685\n",
      "0.007317217066884041\n",
      "0.007275159936398268\n",
      "0.007045974489301443\n",
      "0.007205917499959469\n",
      "0.006788595113903284\n",
      "0.006644203327596188\n",
      "0.006482924800366163\n",
      "0.0064432621002197266\n",
      "0.006258669309318066\n",
      "0.0060765123926103115\n",
      "0.006106960587203503\n",
      "0.0059916917234659195\n",
      "0.005692700389772654\n",
      "0.005686743184924126\n",
      "0.005502649582922459\n",
      "0.00551061425358057\n",
      "0.005395092070102692\n",
      "0.005325683392584324\n",
      "0.005164287984371185\n",
      "0.005069877486675978\n",
      "0.005049072206020355\n",
      "0.005216455087065697\n",
      "0.004911255091428757\n",
      "0.004846156109124422\n",
      "0.004838914610445499\n",
      "0.0046876962296664715\n",
      "0.004775899928063154\n",
      "0.004653807729482651\n",
      "0.004683793988078833\n",
      "0.004538883455097675\n",
      "0.004552751779556274\n",
      "0.004314345773309469\n",
      "0.004386396612972021\n",
      "0.004719952121376991\n",
      "0.00428122328594327\n",
      "0.00431527104228735\n",
      "0.004270084667950869\n",
      "0.0045753829181194305\n",
      "0.0044023143127560616\n",
      "0.0039484151639044285\n",
      "0.004103608895093203\n",
      "0.003980270121246576\n",
      "0.004027445800602436\n",
      "0.003860018216073513\n",
      "0.004099435638636351\n",
      "0.00402242923155427\n",
      "0.00400903495028615\n",
      "0.003926029894500971\n",
      "0.004093554336577654\n",
      "0.0038519182708114386\n",
      "0.0038985456340014935\n",
      "0.003817212302237749\n",
      "0.0038891909644007683\n",
      "0.003999321721494198\n",
      "0.0036994374822825193\n",
      "0.0037628228310495615\n",
      "0.003779547056183219\n",
      "0.0037352745421230793\n",
      "0.0033358456566929817\n",
      "0.0034196926280856133\n",
      "0.0033775519113987684\n",
      "0.003626168705523014\n",
      "0.0037561526987701654\n",
      "0.003375768195837736\n",
      "0.0033326498232781887\n",
      "0.003390268422663212\n",
      "0.003487239358946681\n",
      "0.003693197388201952\n",
      "0.003338973969221115\n",
      "0.0032719268929213285\n",
      "0.003340723691508174\n",
      "0.0032926988787949085\n",
      "0.0035470020957291126\n",
      "0.0031922522466629744\n",
      "0.0030118536669760942\n",
      "0.003210296155884862\n",
      "0.0032443099189549685\n",
      "0.0032040977384895086\n",
      "0.003025318728759885\n",
      "0.003047945909202099\n",
      "0.0030436876695603132\n",
      "0.002978528616949916\n",
      "0.002899398095905781\n",
      "0.0031419305596500635\n",
      "0.0031651146709918976\n",
      "0.003196794306859374\n",
      "0.0031828777864575386\n",
      "0.003045456251129508\n",
      "0.0027709477581083775\n",
      "0.0027614443097263575\n",
      "0.002902908716350794\n",
      "0.0027292536105960608\n",
      "0.0027185198850929737\n",
      "0.0030084624886512756\n",
      "0.0029146703891456127\n",
      "0.002915182849392295\n",
      "0.0028259875252842903\n",
      "0.002645886968821287\n",
      "0.0028120731003582478\n",
      "0.0026265671476721764\n",
      "0.0026201559230685234\n",
      "0.002672688104212284\n",
      "0.002663154387846589\n",
      "0.0027083551976829767\n",
      "0.0028926916420459747\n",
      "0.002741952193900943\n",
      "0.002862389199435711\n",
      "0.0027345987036824226\n",
      "0.0026617725379765034\n",
      "0.0028808144852519035\n",
      "0.00271618808619678\n",
      "0.0023771042469888926\n",
      "0.0023528775200247765\n",
      "0.0024230200797319412\n",
      "0.0023641183506697416\n",
      "0.002296537859365344\n",
      "0.002468232996761799\n",
      "0.0024287784472107887\n",
      "0.002353665651753545\n",
      "0.002435987116768956\n",
      "0.0024102365132421255\n",
      "0.0023811152204871178\n",
      "0.0024012273643165827\n",
      "0.0024564904160797596\n",
      "0.002683719852939248\n",
      "0.002343375701457262\n",
      "0.002218610839918256\n",
      "0.0024921256117522717\n",
      "0.0026011543814092875\n",
      "0.0025575244799256325\n",
      "0.0024000343400985003\n",
      "0.0022026572842150927\n",
      "0.0020304194185882807\n",
      "0.0021911405492573977\n",
      "0.0023999346885830164\n",
      "0.0025697683449834585\n",
      "0.0020852263551205397\n",
      "0.002451887121424079\n",
      "0.002033000811934471\n",
      "0.0022275608498603106\n",
      "0.002195281209424138\n",
      "0.0023081626277416945\n",
      "0.001997096696868539\n",
      "0.0019263450521975756\n",
      "0.0022884488571435213\n",
      "0.002139696618542075\n",
      "0.0022756997495889664\n",
      "0.0022826262284070253\n",
      "0.0023628943599760532\n",
      "0.0020622704178094864\n",
      "0.0019455383298918605\n",
      "0.0019510762067511678\n",
      "0.001975948689505458\n",
      "0.0019636587239801884\n",
      "0.0018800449324771762\n",
      "0.001863132114522159\n",
      "0.0018757919315248728\n",
      "0.0018379604443907738\n",
      "0.0018494522664695978\n",
      "0.0017712839180603623\n",
      "0.0019176738569512963\n",
      "0.002092568902298808\n",
      "0.0018093225080519915\n",
      "0.0019504239317029715\n",
      "0.0017921699909493327\n",
      "0.0019152762833982706\n",
      "0.0018935962580144405\n",
      "0.0017564452718943357\n",
      "0.0020151555072516203\n",
      "0.002096601063385606\n",
      "0.001721818814985454\n",
      "0.0016839613672345877\n",
      "0.002006198512390256\n",
      "0.001789496629498899\n",
      "0.0017076833173632622\n",
      "0.0018188708927482367\n",
      "0.001623240066692233\n",
      "0.0016117654740810394\n",
      "0.0018048391211777925\n",
      "0.0016266488237306476\n",
      "0.0018793605268001556\n",
      "0.002072019036859274\n",
      "0.00145596987567842\n",
      "0.0015333117917180061\n",
      "0.0016352753154933453\n",
      "0.001638207701034844\n",
      "0.001638738438487053\n",
      "0.0017417159397155046\n",
      "0.0016694271471351385\n",
      "0.0018772402545437217\n",
      "0.0016291332431137562\n",
      "0.0018089513760060072\n",
      "0.0019664994906634092\n",
      "0.0018616252345964313\n",
      "0.001377539592795074\n",
      "0.0015719683142378926\n",
      "0.0015268767019733787\n",
      "0.0014787514228373766\n",
      "0.0016584460390731692\n",
      "0.0014567121397703886\n",
      "0.0014141349820420146\n",
      "0.001721691689454019\n",
      "0.0015863822773098946\n",
      "0.001493006362579763\n",
      "0.0014519228134304285\n",
      "0.0014912120532244444\n",
      "0.0015442395815625787\n",
      "0.0018905411707237363\n",
      "0.001479813363403082\n",
      "0.00141212809830904\n",
      "0.0015478826826438308\n",
      "0.0015891704242676497\n",
      "0.001862503937445581\n",
      "0.001544258208014071\n",
      "0.0016150566516444087\n",
      "0.001621021656319499\n",
      "0.00130336987785995\n",
      "0.0016697855899110436\n",
      "0.0016751672374084592\n",
      "0.0014256108552217484\n",
      "0.001397324725985527\n",
      "0.0015448711346834898\n",
      "0.0014053023187443614\n",
      "0.001299454947002232\n",
      "0.001228051958605647\n",
      "0.0014589325292035937\n",
      "0.0012588197132572532\n",
      "0.0013391923857852817\n",
      "0.0013271658681333065\n",
      "0.001462070271372795\n",
      "0.0013174376217648387\n",
      "0.0012223398080095649\n",
      "0.001289783394895494\n",
      "0.0012730133021250367\n",
      "0.0011833520838990808\n",
      "0.0012264613760635257\n",
      "0.0012688759015873075\n",
      "0.0012183288345113397\n",
      "0.001388740842230618\n",
      "0.0012551845284178853\n",
      "0.001284219790250063\n",
      "0.0011493224883452058\n",
      "0.0012963933404535055\n",
      "0.0013628937304019928\n",
      "0.0012917390558868647\n",
      "0.001311725820414722\n",
      "0.0011351217981427908\n",
      "0.001298626884818077\n",
      "0.0012132070260122418\n",
      "0.00140410999301821\n",
      "0.0012559079332277179\n",
      "0.0012215117458254099\n",
      "0.0012786894803866744\n",
      "0.001419174368493259\n",
      "0.0011512303026393056\n",
      "0.0011222705943509936\n",
      "0.001163835171610117\n",
      "0.0012971254764124751\n",
      "0.001026561832986772\n",
      "0.0013349780347198248\n",
      "0.0012079398147761822\n",
      "0.0012253179447725415\n",
      "0.0012921688612550497\n",
      "0.001060068723745644\n",
      "0.0012982637854292989\n",
      "0.0011324243387207389\n",
      "0.0010984709952026606\n",
      "0.001304969540797174\n",
      "0.0010645602596923709\n",
      "0.0012424219166859984\n",
      "0.0010841882321983576\n",
      "0.0010869803372770548\n",
      "0.0011290594702586532\n",
      "0.0009951777756214142\n",
      "0.0012393519282341003\n",
      "0.001163740991614759\n",
      "0.0009758656960912049\n",
      "0.0012721993261948228\n",
      "0.0010651116026565433\n",
      "0.0009979577735066414\n",
      "0.0009869838831946254\n",
      "0.0012446354376152158\n",
      "0.0010195134673267603\n",
      "0.0011481059482321143\n",
      "0.0012222666991874576\n",
      "0.0009345994330942631\n",
      "0.0011816873447969556\n",
      "0.00108623958658427\n",
      "0.0010284496238455176\n",
      "0.001000867923721671\n",
      "0.0009461866575293243\n",
      "0.001169670489616692\n",
      "0.0011482501868158579\n",
      "0.0010133242467418313\n",
      "0.0010155269410461187\n",
      "0.0009634590242058039\n",
      "0.0011924166465178132\n",
      "0.0011375650065019727\n",
      "0.0009840582497417927\n",
      "0.0011778196785598993\n",
      "0.0011265660868957639\n",
      "0.0011934104841202497\n",
      "0.0010893899016082287\n",
      "0.0010643239365890622\n",
      "0.001059414353221655\n",
      "0.001030842075124383\n",
      "0.0011710523394867778\n",
      "0.0009258553618565202\n",
      "0.001088166143745184\n",
      "0.000836724997498095\n",
      "0.001006297767162323\n",
      "0.00115526735316962\n",
      "0.0008507114253006876\n",
      "0.0010921837529167533\n",
      "0.000987761770375073\n",
      "0.0011446366552263498\n",
      "0.000903285457752645\n",
      "0.001135461963713169\n",
      "0.0008671360556036234\n",
      "0.0009132206323556602\n",
      "0.0009643880184739828\n",
      "0.000982928671874106\n",
      "0.0009415691602043808\n",
      "0.0010285332100465894\n",
      "0.0009195597958751023\n",
      "0.0009877578122541308\n",
      "0.0008794284076429904\n",
      "0.0008520747069269419\n",
      "0.0009218935738317668\n",
      "0.0010503241792321205\n",
      "0.0007721324218437076\n",
      "0.0008814785396680236\n",
      "0.0010561207309365273\n",
      "0.0007961237570270896\n",
      "0.0008576197433285415\n",
      "0.0008696322911418974\n",
      "0.001004075980745256\n",
      "0.0009512025280855596\n",
      "0.00078261096496135\n",
      "0.0009315707720816135\n",
      "0.0009746330324560404\n",
      "0.0008648212533444166\n",
      "0.0009516772697679698\n",
      "0.0009422078146599233\n",
      "0.0007786914356984198\n",
      "0.0008691595867276192\n",
      "0.0008895854698494077\n",
      "0.0009925066260620952\n",
      "0.0007545119151473045\n",
      "0.0008846766431815922\n",
      "0.0008143201703205705\n",
      "0.0007998708751983941\n",
      "0.0008543478907085955\n",
      "0.0007971660816110671\n",
      "0.0008359213825315237\n",
      "0.0009653090965002775\n",
      "0.0007374712149612606\n",
      "0.000918281904887408\n",
      "0.0008317853207699955\n",
      "0.000800293346401304\n",
      "0.0007719394052401185\n",
      "0.0007999909576028585\n",
      "0.0008565373136661947\n",
      "0.0009153453866019845\n",
      "0.0007454419392161071\n",
      "0.0007907695253379643\n",
      "0.000821472960524261\n",
      "0.0008135819807648659\n",
      "0.0008225583587773144\n",
      "0.0007714109378866851\n",
      "0.0010525528341531754\n",
      "0.0007276356336660683\n",
      "0.000979496049694717\n",
      "0.0006350657204166055\n",
      "0.0008052835473790765\n",
      "0.0008990933420136571\n",
      "0.0007284088060259819\n",
      "0.0009578547906130552\n",
      "0.0006487005157396197\n",
      "0.000842458859551698\n",
      "0.0007661469862796366\n",
      "0.0008683716296218336\n",
      "0.0007009140099398792\n",
      "0.0007079624920152128\n",
      "0.0008304419461637735\n",
      "0.0008824553806334734\n",
      "0.0007266637985594571\n",
      "0.0007260389975272119\n",
      "0.0008252265397459269\n",
      "0.0007251788629218936\n",
      "0.0008033491903916001\n",
      "0.0008105974993668497\n",
      "0.0006697563221678138\n",
      "0.0007263422012329102\n",
      "0.0008671472314745188\n",
      "0.000745949218980968\n",
      "0.0008608975913375616\n",
      "0.0007775721023790538\n",
      "0.0008023783448152244\n",
      "0.0007726923213340342\n",
      "0.0008594723767600954\n",
      "0.0007979099173098803\n",
      "0.000621893210336566\n",
      "0.0007993732579052448\n",
      "0.0006736230570822954\n",
      "0.0008567849290557206\n",
      "0.0006010634824633598\n",
      "0.000755195680540055\n",
      "0.0006239188951440156\n",
      "0.0007132696337066591\n",
      "0.0007453901926055551\n",
      "0.0006643574452027678\n",
      "0.0007040809723548591\n",
      "0.0006941720494069159\n",
      "0.0007980033406056464\n",
      "0.0005977860419079661\n",
      "0.0008110368507914245\n",
      "0.0007786753703840077\n",
      "0.0007365717901848257\n",
      "0.0006780598196201026\n",
      "0.000644188781734556\n",
      "0.0006973646231926978\n",
      "0.0007475744932889938\n",
      "0.0006756053189747036\n",
      "0.0007967412238940597\n",
      "0.0007862254860810935\n",
      "0.0006600737106055021\n",
      "0.0007053541485220194\n",
      "0.0006715019117109478\n",
      "0.0006316654616966844\n",
      "0.0007371230167336762\n",
      "0.0007635648944415152\n",
      "0.0006787515012547374\n",
      "0.0007687481120228767\n",
      "0.0007483295630663633\n",
      "0.0007139793015085161\n",
      "0.0006627640104852617\n",
      "0.000592464639339596\n",
      "0.0007245544693432748\n",
      "0.0007646900485269725\n",
      "0.000666589941829443\n",
      "0.0006582795176655054\n",
      "0.00066205277107656\n",
      "0.0006059841834940016\n",
      "0.0006715803174301982\n",
      "0.0006394420051947236\n",
      "0.0006021552253514528\n",
      "0.0007052116561681032\n",
      "0.00059722043806687\n",
      "0.0008823947864584625\n",
      "0.0006063127657398582\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(transformer.parameters(), lr=1e-4)\n",
    "n_epochs = 500\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in trange(n_epochs): \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        sequences = batch[1]\n",
    "        tokens, mask, _, _, _ = batch_encode_sequences(sequences)\n",
    "\n",
    "        x = batch[0]\n",
    "        if embedder != \"esmfold\":\n",
    "            x = x[:, 1:-1, :]\n",
    "        mask = mask.bool()\n",
    "        x, mask = x.to(device), mask.to(device)\n",
    "        \n",
    "        # noise = torch.randn_like(x)\n",
    "        # x_noised = x + noise\n",
    "        # output = transformer(x_noised, mask)\n",
    "        output = transformer(x, mask)\n",
    "        loss = F.mse_loss(x, output)\n",
    "        losses.append(loss.item())\n",
    "        if i % 50 == 0: print(loss.item()) \n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3ed73124-df33-40d0-9091-14d34c66bb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde50ede9f0>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56d600-c7a2-4d2a-9209-dbd385cd5a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
