#!/usr/bin/env bash

#SBATCH --job-name scaling 
#SBATCH --nodes 1 
#SBATCH --gpus-per-node 1 
#SBATCH --partition gpu2
#SBATCH --cpus-per-gpu 4 
#SBATCH --mem 100G
#SBATCH --time=15-00:00:00

eval "$(conda shell.bash hook)"

micromamba activate omegafold

echo "SLURM_JOB_NODELIST = ${SLURM_JOB_NODELIST}"
echo "SLURM_JOB_ID = ${SLURM_JOB_ID}"
echo "SLURMD_NODENAME = ${SLURMD_NODENAME}"
echo "SLURM_JOB_NUM_NODES = ${SLURM_JOB_NUM_NODES}"

export HYDRA_FULL_ERROR=1

cd /homefs/home/lux70/code/plaid/pipeline

nvidia-smi

model_id=$1
length=$2

srun python run_pipeline.py \
    ++sample.num_samples=64 \
    ++sample.length=$length \
    ++sample.model_id=$model_id \
    ++sample.output_root_dir=/data/lux70/plaid/artifacts/samples/scaling/${model_id}/$length \
    ++inverse_generate_sequence.max_length=650 \
    ++inverse_generate_structure.max_seq_len=650 \
    ++phantom_generate_sequence.max_length=650 


